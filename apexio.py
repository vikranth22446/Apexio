# -*- coding: utf-8 -*-
"""Copy of Apexio.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1gHfA3vScO_ZdhSZErH3AXmHq011DM8Pr

extensions to edit: .pdf, .txt, .docx
"""

# Commented out IPython magic to ensure Python compatibility.
# import stuff
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

import numpy as np
import random as rand
import os, re, json, nltk, argparse, random, torch, pickle
import tensorflow as tf
import tensorflow_hub as hub
import json
from random import randint
from tensorflow.keras.layers import Input, Dense, Add, Dropout, Concatenate
from tensorflow.keras.models import Model
from models import InferSent
import textract

def get_doc2vec(text, model):
	sents = nltk.sent_tokenize(text)
	emb = model.encode(sents)
	sum = None
	for e in emb:
		if sum is None:
			sum = e
		else:
			sum += e
	return sum / len(sents)

def generate_pairs(dat):
	pairs = []
	for i in range(len(dat)):
		for j in range(i+1,len(dat)):
			pairs.append((dat[i],dat[j]))
	return pairs

def generate_X_y_from_pairs(pairs):
	X = [[],[]]
	y = []
	for pair in pairs:
		X[0].append(pair[0][3])
		X[1].append(pair[1][3])
		y.append(1.0 if pair[0][4] == pair[1][4] else 0)
	return [np.array(x) for x in X], np.array(y)

def generate_X_y_from_dat(dat):
	X = [[],[]]
	y = []
	for i in range(len(dat)):
		for j in range(i+1,len(dat)):
			X[0].append(dat[i][3])
			X[1].append(dat[j][3])
			y.append(1.0 if dat[i][4] == dat[j][4] else 0)
	return [np.array(x) for x in X], np.array(y)

def generate_X_y_for_dir(dat, dir_name):
	X = []
	y = []
	for i in range(len(dat)):
		X.append(dat[i][3])
		y.append(1.0 if dat[i][4] == dir_name else 0)
	return np.array(X), np.array(y)

def get_dirs(path):
	dirs = []
	for r, d, f in os.walk(path):
		if len(d) != 0:
			dirs += d
	return dirs

def generate_quads(names, contents, dirs):
	tarr = []
	for i in range(len(names)):
		ext = names[i].split(r'.')[1]
		try:
			vecs.append(get_doc2vec(contents[i]))
			tarr.append([names[i], ext, contents[i], vecs[-1],dirs[i]])
		except error:
			print(f'File {r} not readable', error)
	return tarr

def generate_model():
	input_1 = Input((4096,), dtype=tf.float32)
	reduce_dim_1 = Dense(2000, activation='relu')(input_1)
	x_1 = Dense(1000, activation='relu')(reduce_dim_1)
	x_2 = Dense(500, activation='relu')(x_1)
	x_3 = Dense(100, activation='relu')(x_2)
	x_4 = Dense(25, activation='relu')(x_3)
	out = Dense(1, activation='sigmoid')(x_4)
	dual_model = Model(inputs=input_1, outputs=out)
	dual_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
	return dual_model

def extract(file_path):
    text = textract.process(file_path)
    return text.decode('utf-8')

def grab_files(path):
	files = []
	for r, d, f in os.walk(path):
		for file in f:
			files.append(os.path.join(r, file))
	return files

def get_files(path, dir, model):
	tarr = []
	files = grab_files(path)
	for f in files:
		#r = f.split(r'/'+dir+'/')[1]
		#ext = r.split(r'.')[1]
		try:
			extr = extract(f)
			tarr.append([f, "NAN", extr, get_doc2vec(extr, model), dir])
		except Exception as error:
			print(f'File {f} not readable', error)
	return tarr

def generate_Infersent_model():
	# Load model

	model_version = 1
	MODEL_PATH = "encoder/infersent%s.pkl" % model_version
	params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,
					'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}
	model = InferSent(params_model)
	model.load_state_dict(torch.load(MODEL_PATH))

	W2V_PATH = 'GloVe/glove.840B.300d.txt'
	model.set_w2v_path(W2V_PATH)

	model.build_vocab_k_words(K=100000)

	return model

def generate_models(dir_tuples, model):

	nltk.download('punkt')

	# This is the big file so run this once to download to your drive
	#!mkdir 'drive/My Drive/ApexioData/GloVe'
	#!curl -Lo 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.zip' http://nlp.stanford.edu/data/glove.840B.300d.zip
	#!unzip 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.zip' -d 'drive/My Drive/ApexioData/GloVe/'

	# Run every time to download on the colab runtime
	#!mkdir encoder
	#!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl

	# Fill with tuples of format (Directory_path, Directory_name) from user settings in application. For the automated folders.

	file_5s = []
	for tup in dir_tuples:
		print('Loading files from:', tup[1])
		file_5s += get_files(tup[0], tup[1], model)
	model_locs = []
	for tup in dir_tuples:
		print('Creating model for', tup[1])
		X, y = generate_X_y_for_dir(file_5s, tup[1])
		m = generate_model()
		m.fit(X, y, epochs=10, verbose=0)
		model_location = f'models/{tup[1]}_model.h5'
		model_locs.append(model_location)
		# Immediately saves model after training
		m.save_weights(model_location)
	return model_locs
