{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apexio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "442uMGyK_uOr",
        "colab_type": "text"
      },
      "source": [
        "extensions to edit: .pdf, .txt, .docx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPM4ruxZAGXB",
        "colab_type": "code",
        "outputId": "5da12fe3-963b-4477-edff-b4bf754ad787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = 'drive/My Drive/ApexioData/'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hLAUZAQeybD",
        "colab_type": "code",
        "outputId": "935eb3fb-af88-4dbc-e9d9-cbd4a1cc77a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# import stuff\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kozJzlZfe3w1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the big file so run this once to download to your drive\n",
        "#!mkdir 'drive/My Drive/ApexioData/GloVe'\n",
        "#!curl -Lo 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.zip' http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "#!unzip 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.zip' -d 'drive/My Drive/ApexioData/GloVe/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnJvYkYFe6-j",
        "colab_type": "code",
        "outputId": "eba18d7b-470c-488b-c762-f231c8a91b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Run every time to download on the colab runtime\n",
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘encoder’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  14.5M      0  0:00:10  0:00:10 --:--:-- 17.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAJ7258te9NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (c) 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "This file contains the definition of encoders used in https://arxiv.org/pdf/1705.02364.pdf\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class InferSent(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(InferSent, self).__init__()\n",
        "        self.bsize = config['bsize']\n",
        "        self.word_emb_dim = config['word_emb_dim']\n",
        "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
        "        self.pool_type = config['pool_type']\n",
        "        self.dpout_model = config['dpout_model']\n",
        "        self.version = 1 if 'version' not in config else config['version']\n",
        "\n",
        "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
        "                                bidirectional=True, dropout=self.dpout_model)\n",
        "\n",
        "        assert self.version in [1, 2]\n",
        "        if self.version == 1:\n",
        "            self.bos = '<s>'\n",
        "            self.eos = '</s>'\n",
        "            self.max_pad = True\n",
        "            self.moses_tok = False\n",
        "        elif self.version == 2:\n",
        "            self.bos = '<p>'\n",
        "            self.eos = '</p>'\n",
        "            self.max_pad = False\n",
        "            self.moses_tok = True\n",
        "\n",
        "    def is_cuda(self):\n",
        "        # either all weights are on cpu or they are on gpu\n",
        "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
        "\n",
        "    def forward(self, sent_tuple):\n",
        "        # sent_len: [max_len, ..., min_len] (bsize)\n",
        "        # sent: (seqlen x bsize x worddim)\n",
        "        sent, sent_len = sent_tuple\n",
        "\n",
        "        # Sort by length (keep idx)\n",
        "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
        "        sent_len_sorted = sent_len_sorted.copy()\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_sort)\n",
        "        sent = sent.index_select(1, idx_sort)\n",
        "\n",
        "        # Handling padding in Recurrent Networks\n",
        "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
        "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
        "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
        "\n",
        "        # Un-sort by length\n",
        "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_unsort)\n",
        "        sent_output = sent_output.index_select(1, idx_unsort)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pool_type == \"mean\":\n",
        "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
        "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
        "            emb = emb / sent_len.expand_as(emb)\n",
        "        elif self.pool_type == \"max\":\n",
        "            if not self.max_pad:\n",
        "                sent_output[sent_output == 0] = -1e9\n",
        "            emb = torch.max(sent_output, 0)[0]\n",
        "            if emb.ndimension() == 3:\n",
        "                emb = emb.squeeze(0)\n",
        "                assert emb.ndimension() == 2\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def set_w2v_path(self, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "\n",
        "    def get_word_dict(self, sentences, tokenize=True):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.bos] = ''\n",
        "        word_dict[self.eos] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in word_dict:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "    def get_w2v_k(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with k first w2v vectors\n",
        "        k = 0\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if k <= K:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    k += 1\n",
        "                if k > K:\n",
        "                    if word in [self.bos, self.eos]:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "\n",
        "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
        "                    break\n",
        "        return word_vec\n",
        "\n",
        "    def build_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "        self.word_vec = self.get_w2v(word_dict)\n",
        "        print('Vocab size : %s' % (len(self.word_vec)))\n",
        "\n",
        "    # build w2v vocab with k most frequent words\n",
        "    def build_vocab_k_words(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        self.word_vec = self.get_w2v_k(K)\n",
        "        print('Vocab size : %s' % (K))\n",
        "\n",
        "    def update_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
        "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "\n",
        "        # keep only new words\n",
        "        for word in self.word_vec:\n",
        "            if word in word_dict:\n",
        "                del word_dict[word]\n",
        "\n",
        "        # udpate vocabulary\n",
        "        if word_dict:\n",
        "            new_word_vec = self.get_w2v(word_dict)\n",
        "            self.word_vec.update(new_word_vec)\n",
        "        else:\n",
        "            new_word_vec = []\n",
        "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
        "\n",
        "    def get_batch(self, batch):\n",
        "        # sent in batch in decreasing order of lengths\n",
        "        # batch: (bsize, max_len, word_dim)\n",
        "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(len(batch[i])):\n",
        "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
        "\n",
        "        return torch.FloatTensor(embed)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        if self.moses_tok:\n",
        "            s = ' '.join(word_tokenize(s))\n",
        "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
        "            return s.split()\n",
        "        else:\n",
        "            return word_tokenize(s)\n",
        "\n",
        "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
        "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
        "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
        "        n_w = np.sum([len(x) for x in sentences])\n",
        "\n",
        "        # filters words without w2v vectors\n",
        "        for i in range(len(sentences)):\n",
        "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
        "            if not s_f:\n",
        "                import warnings\n",
        "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
        "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
        "                s_f = [self.eos]\n",
        "            sentences[i] = s_f\n",
        "\n",
        "        lengths = np.array([len(s) for s in sentences])\n",
        "        n_wk = np.sum(lengths)\n",
        "        if verbose:\n",
        "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
        "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
        "\n",
        "        # sort by decreasing length\n",
        "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
        "        sentences = np.array(sentences)[idx_sort]\n",
        "\n",
        "        return sentences, lengths, idx_sort\n",
        "\n",
        "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
        "        tic = time.time()\n",
        "        sentences, lengths, idx_sort = self.prepare_samples(\n",
        "                        sentences, bsize, tokenize, verbose)\n",
        "\n",
        "        embeddings = []\n",
        "        for stidx in range(0, len(sentences), bsize):\n",
        "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
        "            if self.is_cuda():\n",
        "                batch = batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
        "            embeddings.append(batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # unsort\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "        embeddings = embeddings[idx_unsort]\n",
        "\n",
        "        if verbose:\n",
        "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
        "                    len(embeddings)/(time.time()-tic),\n",
        "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
        "        return embeddings\n",
        "\n",
        "    def visualize(self, sent, tokenize=True):\n",
        "\n",
        "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
        "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
        "\n",
        "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
        "            import warnings\n",
        "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
        "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
        "        batch = self.get_batch(sent)\n",
        "\n",
        "        if self.is_cuda():\n",
        "            batch = batch.cuda()\n",
        "        output = self.enc_lstm(batch)[0]\n",
        "        output, idxs = torch.max(output, 0)\n",
        "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
        "        idxs = idxs.data.cpu().numpy()\n",
        "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
        "\n",
        "        # visualize model\n",
        "        import matplotlib.pyplot as plt\n",
        "        x = range(len(sent[0]))\n",
        "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
        "        plt.xticks(x, sent[0], rotation=45)\n",
        "        plt.bar(x, y)\n",
        "        plt.ylabel('%')\n",
        "        plt.title('Visualisation of words importance')\n",
        "        plt.show()\n",
        "\n",
        "        return output, idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQci5FnMfCPs",
        "colab_type": "code",
        "outputId": "1552efec-07d3-4471-d8f9-e9a5b401624c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load model\n",
        "\n",
        "model_version = 1\n",
        "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "W2V_PATH = 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.txt'\n",
        "model.set_w2v_path(W2V_PATH)\n",
        "\n",
        "model.build_vocab_k_words(K=100000)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dmF6tq-mUe7",
        "colab_type": "code",
        "outputId": "241b52d8-9348-44cb-d3a5-4bccc0b86bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(get_doc2vec('Hello new world. How are you doin'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.1091588  -0.00877047 -0.00819609 ... -0.02699834 -0.03814263\n",
            "  0.0350906 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQVnTmUVqO2S",
        "colab_type": "code",
        "outputId": "a0303e11-f306-4bb2-b5ee-0a7a0cf86f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "!pip3 install textract"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textract in /usr/local/lib/python3.6/dist-packages (1.6.3)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: SpeechRecognition==3.8.1 in /usr/local/lib/python3.6/dist-packages (from textract) (3.8.1)\n",
            "Requirement already satisfied: python-pptx==0.6.18 in /usr/local/lib/python3.6/dist-packages (from textract) (0.6.18)\n",
            "Requirement already satisfied: xlrd==1.2.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.8.0 in /usr/local/lib/python3.6/dist-packages (from textract) (4.8.0)\n",
            "Requirement already satisfied: EbookLib==0.17.1 in /usr/local/lib/python3.6/dist-packages (from textract) (0.17.1)\n",
            "Requirement already satisfied: extract-msg==0.23.1 in /usr/local/lib/python3.6/dist-packages (from textract) (0.23.1)\n",
            "Requirement already satisfied: argcomplete==1.10.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.10.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.12.0)\n",
            "Requirement already satisfied: docx2txt==0.8 in /usr/local/lib/python3.6/dist-packages (from textract) (0.8)\n",
            "Requirement already satisfied: pdfminer.six==20181108 in /usr/local/lib/python3.6/dist-packages (from textract) (20181108)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (6.2.2)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (1.2.7)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4==4.8.0->textract) (1.9.5)\n",
            "Requirement already satisfied: olefile==0.46 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (0.46)\n",
            "Requirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Requirement already satisfied: imapclient==2.1.0 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (2.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (2.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (3.9.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O3AC06YoIub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textract\n",
        "\n",
        "def extract(file_path):\n",
        "    text = textract.process(file_path)\n",
        "    return text.decode('utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvdzJ7KcqNB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "farr = glob.glob(root_path + \"testfiles/*\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIjLNZMsurN6",
        "colab_type": "code",
        "outputId": "7ba48a24-06eb-4a0c-8013-02e87ceb5516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "import re\n",
        "tarr = []\n",
        "vecs = []\n",
        "for f in farr:\n",
        "  r = f.split(r'/testfiles/')[1]\n",
        "  ext = r.split(r'.')[1]\n",
        "  extr = extract(f)\n",
        "  vecs.append(get_doc2vec(r))\n",
        "  tarr.append([r, ext, extr, vecs[-1]])\n",
        "\n",
        "print(tarr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Copy of AMS 131 Quiz 1 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 1 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nThis statement is Meaningless because something can’t have a probability that is a negative number.\\n\\nThis statement is True because if something has a probability of 0.8 its opposite has a probability of 0.2 and 0.8 is four times 0.2 so it is four times as likely to happen as its opposite.\\n\\n (i) is obviously a better option from an intuition standpoint because you are only reliant on one event happening, whereas (ii) requires the same event as (i)  to occur as well as an additional event, meaning (ii) will always be less likely then (i).\\n\\n\\t(i) Probability :  \\n\\n(ii) Probability :  \\n\\n(ii) is a better choice from an intuitive standpoint because you are essentially betting on the same thing as (i), however you get a mulligan in the event that bet doesn’t work out.\\n\\n(i) Probability :  \\n\\n(ii) There are two possible probabilities in this case depending on if the first card was a diamond or not, I will show the probability for both cases:\\n\\n[First card isn’t a diamond] : \\n\\n[First card is a diamond] :', array([ 0.00826579,  0.1265039 ,  0.04765578, ..., -0.01113703,\n",
            "       -0.01545901,  0.05194587], dtype=float32)], ['Copy of Writing 2 #17 Blog - Rough Draft.docx', 'docx', \"Social Media, A Gift & A Curse\\n\\nBy: Maximilian Alfano-Smith\\n\\nIt was only two decades ago that we lived in a world free of social media, one in which children played outside, teenagers went to parties, and the majority of college students weren’t anxious wrecks. Unfortunately, since social media has come to the forefront of popular culture we have seen degradations in these key areas of growth, resulting in a disillusioned and ill equipped generation. In her book iGen, Jean M. Twenge classifies people born after 1995 as those growing up in the midst of social media, a group she dubs the iGeneration. In her quest to better understand this generation and the unique set of challenges it faces, she conducted multiple interviews and analyzed numerous academic surveys.\\n\\n“I feel like we don’t party as much. People stay in more often.” (ADD CITATION TWENGE 69)\\n\\nIn one of her most eye opening interviews with a 17 year old high school student, Twenge asked what makes his generation different, to which he responded: “I feel like we don’t party as much. People stay in more often. My generation lost interest in socializing in person – they don’t have physical get-togethers, they just text together, and they can stay at home” (ADD CITATION TWENGE 69). It's not just anecdotes that support this student’s point but also data, as a 2015 survey referenced by Twenge found that between 1992 and 2014 the percentage of high school 10th graders who attended one or more parties in the last month had dropped by 18% and saw a drop of 26% among the portion of the same population that got together with their friend’s daily or almost daily.  This trend of preferring digital communication and entertainment over physically engaging with others and going places for fun shows a troubling openness to isolation among many iGeners. While in the short term these students may be okay with this physical isolation, in the long term it can lead to the anxiety and depression that has plagued their generation. \\n\\n\\n\\nTeenagers together, yet all on their phones is a common sight among iGeners | Source: https://www.forbes.com/sites/keithwagstaff/2016/02/28/are-your-kids-addicted-to-their-phones-screenagers-wants-to-help/#7abe2e9663eb\\n\\n\\n\\nThe combination of social media with this growing acceptance of non face to face socialization and entertainment could open up many iGeners to the danger of not developing proper social and emotional skills.  In two different academic papers, “Is Social Media Use for Networking Positive or Negative? … Mental Health” by James H. Liu and “Growing up in the Web of Social Networking: Adolescent Development and Social Media” by Jane L. Hur, we see research indicating the issues that can arise as a result of this combination. Both papers argue that when it comes to youth and young adults using social media for socialization a “rich get richer” situation occurs as those who already have good face to face relationships can find more online, whereas those who struggle socially and may look to social media as an outlet will only further their isolation. However this isn’t the whole story, these papers also found a connection between the detrimental effects of social media and how addicted to the internet someone is. Internet addiction is when someone prefers a digital activity over its physical alternative to the detriment of their social, emotional, or physical health. Like the quality of one’s face to face relationships, both papers found that when someone is addicted to the internet and uses social media they are much more likely to derive feelings of anxiety and depression from it as they can become emotionally dependent upon the weak connections it provides. \\n\\n\\n\\nTeenagers at a party, a sight that becomes rarer everyday | Source: http://www.bashcorner.com/5-bombastic-party-themes-for-teenagers/ \\n\\n\\n\\nThankfully not all hope is lost, like most new things social media has unforeseen consequences and as long as we properly account for these we can construct a society in which everyone can fall into the “rich get richer” category of online socializing. However in order to get to this point schools and parents need to properly educate those under their supervision of the potential pitfalls of social media and how to avoid becoming dependent upon it. It needs to be stressed to the youth that social media is merely a way to find new people, it's not going to make you more social or approachable as interacting with new people face to face will, it's just going to put you in contact with more people than could ever fit in a high school hallway. Once people understand this key distinction between social media and in person communication they can begin using it in much healthier and productive ways.\", array([ 0.00826579,  0.12592942,  0.03430425, ...,  0.01043972,\n",
            "       -0.03123897,  0.09561853], dtype=float32)], ['Copy of AMS 131 Quiz 5 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 5 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n\\n\\nIn order to calculate the normalizing constant we need to use the fact we know if we integrate across the support for both variables we will get 1. So: \\n\\nTo calculate the marginal PDF of X: \\n\\nTo calculate the marginal PDF of Y: \\n\\nThese are the same marginal PDFs as those provided in the quiz, thus proving they are correct.\\n\\nThey are not independent because if they were then multiplying the two marginal PDFs would result in the original PDF, however if we do that here we get: \\n\\nGiven the two equations are not equal, X and Y must not be independent in this joint distribution.', array([ 0.00826579,  0.17426825,  0.04765578, ...,  0.06905699,\n",
            "       -0.02332487,  0.02924924], dtype=float32)], ['Copy of AMS 131 Quiz 2 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 1 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nEach day of the year can occur on one of seven weekdays, however since there are leap years there is an addition day that can also occur on each of the seven weekdays, therefore there a total of 14 possible calendars. The reason leap years and non leap years both contribute 7 combinations each is because in the case of a non-leap year the shift of each day from one week day corresponds to each other day of the year shifting, however when you add an extra day the number of possible shifts is a new set of 7 because each shift in February 29ths weekday shifts every other day. \\n\\n\\n\\nSince no balls have been drawn the probability that you will draw a red ball is the number of red balls divided by the total number of balls: \\n\\nWe have no information about the previously drawn balls, therefore the probability is the same as if no balls have been drawn: \\n\\nWe have no information about the previously drawn balls, therefore the probability is the same as if no balls have been drawn:', array([ 0.00826579,  0.12565431,  0.04765578, ...,  0.01271085,\n",
            "       -0.01673526,  0.0537103 ], dtype=float32)], ['Copy of AMS 131 Quiz 3 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 3 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nThe probability a random person responded yes to the question is the number of people who said yes divided by the total number of people: \\n\\nGiven we know that the chosen person is a female the probability she said yes is the number of females who said yes divided by the total number of females: \\n\\nGiven we know that the chosen person is a male the probability he said yes is the number of males who said yes divided by the total number of males: \\n\\nThey are probabilistically dependent because there is 91% chance a male will be in favor of marijuana legalization whereas the same probability for a woman is only 59%. If they where probabilistically independent then the percentages for the two genders would be much closer together.\\n\\nI would say moderate because while there is definitely a correlation that’s more than just weak. In my mind for it to be strong it should be possible to guess a subject’s gender with relative accuracy given their opinion, however in this data set that isn’t the case.', array([ 0.00826579,  0.16889803,  0.04765578, ...,  0.03198316,\n",
            "       -0.01926284,  0.0412984 ], dtype=float32)], ['Copy of AMS 131 Quiz 4 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 4 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nThe key assumption you are making is that the generalizations provided hold for the convention you are at.\\n\\nP(Sh | St) = 0.8 = 80%, P(Sh | E) = 0.15 = 15%, P(E) = 0.9 = 90%\\n\\nYou have information about the probabilities that are dependent upon another probability. You are then given a probability and need to predict that originally given probability. The best way to do this is using the conditional probability methods provided by Bayes.\\n\\nThis approach is the best because it allows you to calculate the odds without actually calculating the denominator, enabling us to be creatively lazy.\\n\\n1 = (Posterior odds in favor of St over E) , 2 = (Prior odds in favor of St over E), 3 = (Bayes factor in favor of St over E)\\n\\n. You are calculating the odds by multiplying the odds that a given person is a statistician vs economist by the probability that a person is shy given if they’re a statistician or economist; this gives you the odds that they are a statistician vs an economic given they are shy.\\n\\n\\n\\nEven though statisticions are much more likely to be shy, there are 9 times as many economists at the conference. This is similar to the credit card case study where when a card was marked as bad there was a decent chance it was in fact good, even though cards marked good where almost always good. This was caused by a similar situation where the number of truly good cards vastly outweighed the number of truly bad cards.', array([ 0.00826579,  0.18249875,  0.04765578, ...,  0.03697877,\n",
            "       -0.02186014,  0.04085668], dtype=float32)], ['Copy of AMS 131 Quiz 6 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 6 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n\\n\\nThe CDF for y is just the the indefinite integral of the PDF of y from the lower bound of the support to some variable y: \\n\\n\\n\\nUsing an inverse function calculator I found the quantile function to be: \\n\\n\\n\\nYou use a random number generator to generate a random probability across the support, and then when you plug this into the inverse function from (c) and you get an IID random draw.\\n\\nYou take the y values from the inverse cdf from 0 to 1 and then graph them on a histogram and if it has the same distribution as the cdf then it is the correct inverse function.', array([ 0.00826579,  0.16759786,  0.04765578, ...,  0.05762699,\n",
            "       -0.02350705,  0.03436177], dtype=float32)], ['Copy of AMS 131 Quiz 7 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 7 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nWe need to calculate the covariance between W and Z in order to show they are uncorrelated:\\n\\n\\tSince: , we know that these two \\tvalues are equal so the result is zero and the variables W and Z are thus uncorrelated.\\n\\nGiven we know that:\\n\\nWe can further simply , the correlation between X and Y is negative therefore so  as you subtract a negative number rather than adding it.\\n\\nWe need to solve for and we know that , we will now use this to calculate our desired variances: \\n\\n\\n\\n\\n\\nWe will use the basic computational form of variance and incorporate our known values:\\n\\n\\n\\n\\n\\n\\n\\n, the covariance is less than -1 so the RA made an error.', array([ 0.00826579,  0.16255477,  0.04765578, ...,  0.05103614,\n",
            "       -0.02479353,  0.02018769], dtype=float32)], ['Copy of AMS 131 Quiz 9 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 9 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nWe need to solve for the expected value and variance of using the knowledge that . , . Using this we can now work forwarded from the provided inequality, . We now need to show the best way to choose n so that the probability is at least . .\\n\\n\\n\\nBased on the graph in (b) we know that where z is a normally distributed random variable between 0 and 1. Given this the previous probability is greater than or equal to. . So we get the required inequality of .\\n\\n.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.1\\n\\n10\\n\\n2.7\\n\\n0.05\\n\\n20\\n\\n3.8\\n\\n0.01\\n\\n100\\n\\n6.6\\n\\n0.005\\n\\n200\\n\\n7.9\\n\\n0.001\\n\\n1000\\n\\n10.5\\n\\nIt is very conservative because given a certain alpha value the Chebyshev Inequality uses way more samples than a normal distribution.', array([ 0.00826579,  0.13520783,  0.04765578, ...,  0.04962443,\n",
            "       -0.02474811,  0.02180099], dtype=float32)], ['Copy of AMS 131 Quiz 10 Responses.docx', 'docx', 'AMS 131\\n\\nQuiz 10 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nThis is a time-homogeneous Markov chain because we assume that are all constant through time. It is a Markov chain because in order to understand the current state of the system all you need is its state one time step back.\\n\\nThis is the correct transition matrix because for a given previous time step the resulting value when passed through this matrix is the probability of the first player playing a different opponent. Additionally the rows need to add up to 1 and they do because each one contains both the probability of two players playing and the probability of them not playing, which adds up to 1.\\n\\nUsing wolfram alpha we get that . Now we need to divide by the sum of this vector so it adds up to 1. . Now we need to multiply this into the matrix to show its the correct equilibrium distribution. \\n\\nThis distribution makes sense as even though it may appear like X and Y with be playing 49% of the time, and X and Z will be playing 30% of the time, in reality all players will get a good amount of play time given this distribution as two players can’t play each other consecutively.\\n\\nIf they are all equally good then the equilibrium distribution should be and given they are all evenly good so we can verify our equilibrium distribution through eigen-analysis. . The analysis confirms this is the correct distribution.', array([ 0.00826579,  0.12565431,  0.04765578, ...,  0.09129906,\n",
            "       -0.02919903,  0.01824412], dtype=float32)], ['Copy of AMS 131 Quiz 8 Responses.docx', 'docx', \"AMS 131\\n\\nQuiz 8 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\nThe PMF for X is: .\\n\\nThe expected utility function is: \\n\\n\\n\\n\\n\\nYou should chose B = 0 because you are more likely to lose B dollars than win B dollars, so you are most likely going to lose money if .\\n\\nIn order to maximize the expected utility function with respect to B we need to take the derivative of the expected utility function with respect to B: \\n\\n. This answer makes sense because as p approaches 1 your probability of winning goes up, so you should bet a larger percentage of A. Additionally if p = 1, you should bet all your money as you're guaranteed to win and when you plug p = 1 into this equation you get B = A.\", array([ 0.00826579,  0.145209  ,  0.04765578, ...,  0.05839702,\n",
            "       -0.0260386 ,  0.02453738], dtype=float32)], ['Copy of Writing 2 #18 Blog - Final Draft.docx', 'docx', \"Social Media, A Gift & A Curse\\n\\nBy: Maximilian Alfano-Smith\\n\\nIt was only two decades ago that we lived in a world free of social media, one in which children played outside, teenagers went to parties, and the majority of college students weren’t anxious wrecks. Unfortunately, since social media has come to the forefront of popular culture we have seen degradations in these key areas of growth, resulting in a disillusioned and ill equipped generation. In her book iGen, Jean M. Twenge classifies people born after 1995 as those growing up in the midst of social media, a group she dubs the iGeneration. In her quest to better understand this generation and the unique set of challenges it faces, she conducted multiple interviews and analyzed numerous academic surveys.\\n\\n“I feel like we don’t party as much. People stay in more often.” (iGen 69)\\n\\nIn one of her most eye opening interviews with a 17 year old high school student, Twenge asked what makes his generation different, to which he responded: “I feel like we don’t party as much. People stay in more often. My generation lost interest in socializing in person – they don’t have physical get-togethers, they just text together, and they can stay at home” (iGen 69). It's not just anecdotes that support this student’s point but also data, as a 2015 survey referenced by Twenge found that between 1992 and 2014 the percentage of high school 10th graders who attended one or more parties in the last month had dropped by 18% and saw a drop of 26% among the portion of the same population that got together with their friend’s daily or almost daily. This trend of preferring digital communication and entertainment over physically engaging with others and going places for fun shows a troubling openness to isolation among many iGeners. While in the short term these students may be okay with this physical isolation, in the long term it can lead to the anxiety and depression that has plagued their generation. \\n\\n\\n\\nTeenagers together, yet all on their phones is a common sight among iGeners | Source: https://www.forbes.com/sites/keithwagstaff/2016/02/28/are-your-kids-addicted-to-their-phones-screenagers-wants-to-help/#7abe2e9663eb\\n\\n\\n\\nThe combination of social media with this growing acceptance of non face to face socialization and entertainment could open up many iGeners to the danger of not developing proper social and emotional skills.  In two different academic papers, “Is Social Media Use for Networking Positive or Negative? … Mental Health” by James H. Liu and “Growing up in the Web of Social Networking: Adolescent Development and Social Media” by Jane L. Hur, we see research indicating the issues that can arise as a result of this combination. Both papers argue that when it comes to youth and young adults using social media for socialization a “rich get richer” situation occurs as those who already have good face to face relationships can find more online, whereas those who struggle socially and may look to social media as an outlet will only further their isolation. However this isn’t the whole story, these papers also found a connection between the detrimental effects of social media and how addicted to the internet someone is. Internet addiction is when someone prefers a digital activity over its physical alternative to the detriment of their social, emotional, or physical health. Like the quality of one’s face to face relationships, both papers found that when someone is addicted to the internet and uses social media they are much more likely to derive feelings of anxiety and depression from it as they can become emotionally dependent upon the weak connections it provides. \\n\\n\\n\\nTeenagers at a party, a sight that becomes rarer everyday | Source: http://www.bashcorner.com/5-bombastic-party-themes-for-teenagers/ \\n\\n\\n\\nThankfully not all hope is lost, like most new things social media has unforeseen consequences and as long as we properly account for these we can construct a society in which everyone can fall into the “rich get richer” category of online socializing. However in order to get to this point schools and parents need to properly educate those under their supervision of the potential pitfalls of social media and how to avoid becoming dependent upon it. It needs to be stressed to the youth that social media is merely a way to find new people, it's not going to make you more social or approachable as interacting with new people face to face will, it's just going to put you in contact with more people than could ever fit in a high school hallway. Once people understand this key distinction between social media and in person communication they can begin using it in much healthier and productive ways.\", array([ 0.00826579,  0.12758537,  0.03430425, ...,  0.00365862,\n",
            "       -0.02725148,  0.09559109], dtype=float32)], ['Copy of AMS 131 Test 1 Responses.docx', 'docx', \"AMS 131\\n\\nTest 1 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n(public health)\\n\\nThis is an observational study as you can not force people to smoke in order to conduct a study.\\n\\nFrom table 3: \\n\\nFrom table 1: \\t\\n\\nFrom table 2: \\n\\nIt definitely seems like there is a correlation between age and smoking habits, as 44% of women smoke, 50% of women age 18-64 smoke, and only 20% of women age 65+ smoke. It seems like the correlation is that younger women are more likely to smoke.\\n\\n\\n\\n\\t\\t\\n\\n\\t\\t\\n\\nI feel like you could argue it establishes a connection, however the difference between P(dead) and each of the two categories is only about so you could also argue its very weak. If you were to argue there is a connection it would actually point to smoking increasing morality. However, as I pointed out earlier this is not a very compelling argument given how close together the three values are.\\n\\nFrom table 1: \\n\\nFrom table 1: \\n\\nFrom table 1: \\t\\n\\nFrom table 2: \\n\\nFrom table 2: \\n\\nFrom table 2: \\t\\n\\nAge is a confounding factor because when you don’t take it into account it appears as if smoking makes you live longer. However, when you look at the data in the context of restricted age groups you actually see in both instances that a higher percentage of smokers died than non-smokers. It probably goes in the opposite direction because more young people smoke, but young people are on average healthier than old people thus skewing the results.\\n\\nThe simpson's paradox occurs here because as I pointed out in (d) young people are both on average healthier and more likely to smoke than old people and we have almost five times as many young people as old people in the study; thus causing the incorrect correlation between smoking and living longer. Given how when we split up the age groups we saw smoking causing death in both groups I feel our conclusion in (d) is much more accurate.\\n\\n(gambling)\\n\\nThe odds are not 1 in 35,064,160,560 because that value assumes the order of the balls matters, however this is not the case, therefore you must divide by 5! as you need to negate the inclusion of order in the original calculation. This gives you the same odds as provided in the table: 1 in 292,201,338.\\n\\nThe odds 1 in 11,238,513 are calculated not accounting for the person also getting the red ball incorrect. To get the correct answer you must multiply 11,238,513 by 25/26 in order to account for them getting the wrong red ball. When you do this calculation you get the correct answer of: 1 in 11,688,053.52.\\n\\nThe first formula is correct because you need to determine the combinations within the winning 5 balls of size k, the number of incorrect balls (5 - k) out of the remaining 64 balls, 1 of 1 which is the correct red ball, 0 of 25 which is the remaining red balls, and then divide by the total number of 6 ball combinations. The second formula is correct because you you need to determine the combinations within the winning 5 balls of size k, the number of incorrect balls (5 - k) out of the remaining 64 balls, 0 of 1 which is the correct red ball that wasn’t pulled, 1 of 25 which is the 1 incorrect red ball pulled out of the other 25, and then divide by the total number of 6 ball combinations. \\n\\nOdds from table 4:\\n\\nAll five whites and the red: \\n\\nAll five whites: \\n\\nFour whites and the red: \\n\\nFour whites: \\n\\nThree whites and the red: \\n\\nThree whites: \\n\\nTwo whites and the red: \\n\\nOne white and the red: \\n\\nThe red: \\n\\nGiven any of the outcomes in table 4 will result in a prize greater than $2 the odds of winning a prize is the odds of all of the possible prize outcomes connected with an or, aka you add them together. The statement needs word total, because 1 in 24.87 is the total odds for all of the possible prize outcomes.\\n\\nThe chance of at least one grand prize winner is: \\n\\n\\n\\n\\n\\n(logic and Bayes’s Theorem)\\n\\nHis probability of being pardoned does not change even though the warden provided him with new information, this is because the warden already made his decision randomly before the conversation making this information ultimately irrelevant to his fate. From a bayesian perspective his probability of being pardoned is not changed by the warden’s response as no matter the outcome, whether he is pardoned or not, at least one of his fellow inmates will not be pardoned; this makes the warden’s answer meaningless as he already knew another inmate would not be pardoned and he has no way to use the new information of that inmates identity to change his original odds. Even though it may appear as though new information has been acquired, thus enabling conditional probability, in fact the odds stay the same as we always knew at least one of the other inmates would not be pardoned.\\n\\n(optimal hiring strategy)\\n\\nFor any i > r, when you interview the first i people, the probability that the best candidate is in the first r people is:\\n\\nThe probability P(A|Bᵢ) = 0 for  because you decided you would eliminate the first r interviewees automatically, therefore making it impossible you hire one even if they are the best candidate in the pool. The probability that the i is the best candidate and is hired for i > r is similar to (a) since if the ith person is the best pick and you chose them, that means at least one entry in r is better than all of the entries between r and i, therefore you just need the probability from (a) except instead of dividing by i you divide by i-1 as the best entry in r is only better than all of the candidates up too but not including the ith candidate.\\n\\n\\n\\n\\n\\nthis is true because r is 0, therefore you will always pick the first candidate you meet making it completely random as too if they are the best candidate.\\n\\nThe probability for a given r is found by connecting the probability of each candidate after r being the best with or, and then dividing by the total number of candidates n. This can be written out as:=\\n\\nGiven that therefore:\\n\\n\\n\\n The first element in the summation is and the rest of the added elements decrease until i = n. Therefore as r increases the elements of the summation decrease causing the function  to decrease.\\n\\nThe value of is when the value of is less than that of , therefore the largest r such that is also the r that maximizes , as while .\\n\\nThe best r for n = 10 is 3. This does not seem like a very good hiring strategy because even when you calculate the optimal r, you still only have about a 40% chance of hiring the correct candidate as .\\n\\n(portfolio management)\\n\\nWe are given therefore .We know from the definition of CDF that is an integral of the PDF , given this , thus . Since we know the relationship between our two PDFs is , , given this we can rewrite our earlier equation as: . Since is a CDF we know if we integrate it across the entire real number line the result will be 1 so we can further update our equation as . Given we are dealing with a CDF the input value must be -v giving us the expression: . Given this expression we know that .\\n\\nVaR seems very sensitive as its value is pretty varied depending on the PDF you choose. \\n\\n\\n\\nSince we know the function is only non-zero over [-10,20]: . Now that we know c we can define , therefore: . Given the definition we established in (a), and we are given that we can now calculate v: .\\n\\n\\n\\nSince we know the function is only non-zero over [-10,20]: . Now that we know c we can define , therefore: . Given the definition we established in (a), and we are given that we can now calculate v: \\n\\n\\n\\nBased on the information we are given we know that c = 0.005569078782, , and the CDF =. Given the definition we established in (a), and we are given that we can now calculate v: .\\n\\nI feel that the VaR is high sensitive as its value has varied greatly over the five PDFs examined. It is interesting to note however, how (b ii) and (b iii) are relatively close together, as is (c i) and (c ii), while (b i) is pretty much in the middle of the two groups.\\n\\n\\n\\nSince we know the function is only non-zero over [-10,20] we can define our uniform CDF as: . Now that we have our CDF we can calculate v: .\\n\\n\\n\\nSince we know the function is only non-zero over [-10,20]: .  Now that we know c we can define , therefore: . Given the definition we established in (a), and we are given that we can now calculate v: .\", array([ 0.00826579,  0.14047271,  0.04765578, ..., -0.02704611,\n",
            "       -0.00635676,  0.05755174], dtype=float32)], ['Copy of AMS 131 Test 2 Responses(1).docx', 'docx', 'AMS 131\\n\\nTest 2 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n(the Exchange Paradox)\\n\\nGiven we know that the amount of money put in envelope 1 is m, then the probability of you finding m or 2m dollars in your envelope is X, so given we know m we can write the probability of X as: \\n\\nNow if we try to determine the probability of M’s value based on X we can describe that as: \\n\\nWe can use this too write the probability of getting x vs when trading as the odds between the two outcomes: \\n\\n\\n\\nGiven all this we will let Y stand for the amount of money in our opponent’s envelope. So we need an equation for the expected value of Y given you know X: \\n\\nS\\n\\ns\\n\\n(practice with joint, marginal and conditional densities)\\n\\nS\\n\\nS\\n\\nS\\n\\nS\\n\\nThe marginal distribution for y1 is: \\n\\nThe marginal distribution for y2 is:\\n\\ns\\n\\nThey are dependent for two reasons, first because the support requires y2 is less than y1 and second if you multiply the two marginals distributions you don’t get the joint distribution:  \\n\\ns\\n\\n(moment-generating functions)\\n\\nD\\n\\nK\\n\\nWe know that and the MGF of a discrete random variable is: . So given all this the MGF for the discrete random variable Y is: now we have to use the knowledge that and say that giving us: \\n\\n[NOTE] skewness(Y) = \\n\\nD\\n\\nD\\n\\nd\\n\\n(archaeology)\\n\\nD\\n\\nD\\n\\nd', array([ 0.00826579,  0.15818885,  0.04765578, ..., -0.00863594,\n",
            "        0.01949928,  0.09653898], dtype=float32)], ['Copy of AMS 131 Test 3 Responses.docx', 'docx', 'AMS 131\\n\\nTest 3 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n(biology)\\n\\nWe know that for a given element i in the vector : so in order to get the average value of , we need to sum these values for i = 1 to i = n and divide by n: , we know from the problem statement that , so we can finish our previous equation as: . Given  is an IID random variable the variance of  is as its standard deviation is so we can now calculate the variance of : . Using these values we can now calculate the expected value of as: , since is the average of all values in  its expected value should be zero as this number will approach zero as n increases so we can finish defining the above expected value as: . Now we need to calculate the variance of  as: . The variance of should be smaller than that of any given  as it is the average of all values in  and would therefore be much less likely to vary than any given element. Given that tends to zero as n increases, and , will only converge to the truth  when b = 0 as this amount of error doesn’t arrive from random events but instead bias in your method. The typical amount that the expected value of  differs from the truth is defined as the root mean squared error: , however can further simply this given our above information as: \\n\\nas  should be zero given a large enough n. This function also goes to zero when b = 0 and a large enough sample size n is used.\\n\\nAs stated in paragraph four the kit is known to give “unbiased measurements that fluctuate around the true value with a SD of 0.15 and an approximately normal distribution for its measurement errors”. Therefore I can write my measurement and we know that is a normally distributed variable with a SD of 0.15. Given this we need to calculate . We will define a new variable , is now a normally distributed random variable with mean zero and SD 1, I can now use a standard normal distribution table to calculate the above probability. From looking at the table we can conclude the probability of to be 0.2546.\\n\\nWe need to calculate the sample size such that and accomplish this by furthering simplifying this probability as . We know each has a mean of zero and a SD of 0.15 so as we calculated earlier the variance of is . We now know that we need to calculate n such that . . We will now define a new variable which a normal random variable with mean 0 and SD . We can now rewrite our objective probability as . And therefore using a standard normal distribution table must be greater than 2.57. Given all this we now must solve for n and since our table only covers positive z values and the distribution is symetric about zero therefore we can solve for n using, . So you must take at least 15 samples in order for your probability of incorrectly measuring the Ph to be less than 5 to be less than 0.5%.\\n\\n(medicine)\\n\\nGiven the data from the mid-1970s the average effect of Captopril on the systolic blood pressure is to reduce the value by about 18.6 mmHg for people deemed hypertensive. At that point in time hypertensive was defined as a systolic blood pressure of 140 mmHg of mercury or higher. The effect when compared to the average systolic blood pressure of the subjects is about a 10% reduction (). This is medically significant as a 10% difference is quite rare in medicine. In order to calculate the standard error we will use the inverse t equation in our case which gives us a standard error of 6.4 mmHg. Using this we can construct a 95% confidence interval with a lower bound of 12.2 mmHg, center of 18.6 mmHg, and upper bound of 25 mmHg. We are assuming that this group of 12 subjects is representative of the entire hyptertensive population on the United States and that the equipment used to measure their blood pressure is unbiased. This estimated effect is statistically significant because zero change or no effect due to Captopril is not within the 95% confidence interval. Given all this Captopril seems to be a good treatment option for hypertension.\\n\\n\\n\\nWe will define the variance of the completely randomized design as: . We will now do the same for the repeated measures design: \\n\\n. Now we need to find the covariance of using the provided probability . We can now finish our variance calculation: . We will now define the efficiency of RM compared to CR as: . This means the RM model is five times as efficient as the CR model.\\n\\n\\n\\nIt seems to have a stronger effect on people who have a higher initial blood pressure. This is supported by figure 1 as there is a 0.501 correlation between the before blood pressure of patients and their resulting difference after taking the drug. \\n\\n(binomial and negative binomial sampling)\\n\\nWe need to use a Binomial model for S because it matches the sampling method for S. The proportion of 1s in the data is given by, which is a good guess for . The expected value of is as the expected value of  is because S is a binomial distribution. Given this must be unbiased for . The standard error for is . S is defined as therefore . In order to find the variance of we need to find the expected value of it and it squared. , . Now we can calculate the variance of as: . Using this we can finish our calculation of the variance of S: . Using all this we can now calculate the standard error of  as: . should be approximately normal when the skewness goes away so when n is sufficiently large or .\\n\\n\\n\\nGiven X is the total of number of failures before the sth success and s is the number of successful tests, the the total number of tests N is: .\\n\\nIn order to find the PMF of N we need to work forward from the definition of PMF provided for X. \\n\\n. Using the theorem for  we will set , giving us: . This allows us to finish the PMF of N as: .\\n\\nWe know that . Given X is a negative binomial distribution with inputs s and its variance is: which is also the variance of N given our previous findings. Using this and the fact that the expected value of X is:  we can now calculate the expected value of N as: . We know that , this makes calculating the expected value and variance of it very hard so we will use the delta method. We will define a function g and its derivative as that we can now take the expected value of with N as its parameter. so . We will define a new variable W and find its expected value and variance and then sub in into the resulting simpler equations. . We now need to find the variance of g when its parameter is N: . Given this . Using this we can now define the standard error of as: .\\n\\nJensen’s inequality tells us for a linear convex function g(x), , and for a non-linear convex function its the same except it is a hard inequality. so its expected value is: . is a nonlinear convex function so by Jensen’s inequality: . We are assuming without proof that . We now need to find the difference between andso we can later calculate the expected value of this quantity. . Now that we have simplified the difference we can use it to better understand the relationship between and .  . Sincewill always be positive, must be true. Additionally since as goes to zero goes to infinity must be biased on the high side. Therefore as N gets sufficiently large becomes unbiased. The order of bias magnitude for .\\n\\nYes because the expected value and standard error are very similar.\\n\\n(public health)\\n\\nThis helps combat possible non-placebo effects in the population that didn’t receive the vaccine, as if they know they didn’t receive the vaccine they would take more precautions.\\n\\nYou could run this experiment in a double blind fashion by writing down which syringe numbers contain what and then locking this away until the trial period is over. This would help in the case of this question as the person administering the syringe could somehow indicate to the patient that they are receiving a placebo.\\n\\nThis experiment consists of two sample sets, those who were actually treated with the vaccine and those whose received a saline solution in the control group. We will define the random variables for each as follows: vaccine = and control = . Based on the information provided in the problem we know that and . Using this we can calculate the difference between the two quantities as: . Now we need to calculate the standard error of this rough estimation: , . We now need to calculate the variance for each random variable, however the resulting equation will hold for both if we replace the corresponding n and values so I will only show the math for . \\n\\n. Based on this we can finish our above variance calculation as: . We can also now finish our standard error calculation of , . This is a simple pythagorean calculation so we can complete it based on the following values: , so the standard error for  is: . We will now sketch the 99.9% confidence interval for the our approximation , . \\n\\nThis difference is very statistically significant as zero is not in the 99.9% confidence interval and there is a whole interval step between the left endpoint and zero meaning the treatment is highly effective.\\n\\nIn order to find a proper sample size n we must first calculate the distance between our center and 0 in order to find the correct n: . We should construct a scenario in which the treatment and control sample spaces are each half of the total sample space i.e. .We will now refine our above calculation using this fact:\\n\\n. Therefore the optimal sample size is 95,402 kids.', array([ 0.00826579,  0.18251403,  0.04765578, ...,  0.03198316,\n",
            "       -0.00999445,  0.04770049], dtype=float32)], ['Copy of Writing 2 #11 Topic Exploration.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nTopic Exploration\\n\\nFindings:\\n\\nGVRL: Really has nothing of us for this topic.\\n\\nCQ: (Loneliness and Social Isolation by: Christina L. Lyons | http://library.cqpress.com/cqresearcher/document.php?id=cqresrre2018080300&type=hitlist&num=8) and (Social Media Explosion by: Marcia Clemmitt | http://library.cqpress.com/cqresearcher/document.php?id=cqresrre2013012500&type=hitlist&num=3) \\n\\nFurther reading: (Networks of outrage and hope : social movements in the internet age by: Manuel Castells) and (Virtually you : the dangerous powers of the e-personality by Elias Aboujaoude)\\n\\nNews and newspapers: (Facebook, ‘Asleep at the Wheel’, David Leonhardt, NYT | https://www.nytimes.com/2019/04/30/opinion/facebook-political-ads.html?rref=collection%2Ftimestopic%2FSocial%20Media&action=click&contentCollection=timestopics&region=stream&module=stream_unit&version=latest&contentPlacement=6&pgtype=collection) \\n\\nNews link: I didn’t like the ones from my other news piece, so I found one on my own. (How to Take Back Control From Facebook, Charlie Warzel, NYT | https://www.nytimes.com/2019/04/30/opinion/facebook-ftc-privacy.html?rref=collection%2Ftimestopic%2FSocial%20Media&action=click&contentCollection=timestopics&region=stream&module=stream_unit&version=latest&contentPlacement=2&pgtype=collection) \\n\\nWebsite: I chose media matters as its a non-profit that monitors media bias. Every single article on its front page attacks people or organizations on the right. I initially found this a bit off, but after doing a bit more research it seems like almost all disinformation in media comes from the right.\\n\\nProcess: \\n\\nI initially just started with the topic of social media which is quite broad so I tried to find ways to narrow it down. I am still on the fence about what direction I want to go back between the social isolation it causes and political turmoil it also causes. I think I like the social isolation topic better, however I have found it much easier to find sources on political turmoil. I am hoping that in the next few days I will be able to decide which way to take my research, because both are very interesting but I can only do one.\\n\\nQuestions:\\n\\nWhat effect does social media have on the psychological development of youth?\\n\\nHow has social media changed the way Americans engage with politics?\\n\\nHow does our use of social media impact our concepts of self and community?\\n\\nTopic Description:\\n\\n\\tAs I said in process I am currently torn between two specific focuses for the social media topic, but I am definitely locked into writing about social media. I chose this topic because I hate social media, it personally caused me a lot of emotional trouble in middle and high school, and I have been off of it except snapchat since 10th grade. I have also watched multiple people I care about throughout the last few years struggle with the feelings of insecurity and self-loathing that social media inspires. I would really like to learn more about how it has these effects and what we can do to deal with them. In my research I have also found a lot about social media in politics and given the huge amount of information on that I may just end up doing that.', array([ 0.00826579,  0.12592942,  0.03430425, ...,  0.01375966,\n",
            "       -0.02016483,  0.10091437], dtype=float32)], ['Copy of Writing 2 #15 Annotated Bibliography Responses.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nAnnotated Bibliography\\n\\nCain Response: In this article Cain looks at how the emerging mental health crisis among college undergraduate and graduate students is connected to social media use. The first big takeaway of the piece comes from another medical academic named Eiser who argues that social media dulls people’s ability to properly handle conflict by allowing the creation of intellectual safe spaces. This then has a compounding effect with our current political turmoil to create a situation where users are surrounded by affirmation when they stay within their circles and are met by opposition when they leave said circles; this promotes anti-social behaviour as it makes people not want to leave their digital groups. It also causes people to falsely equate words with violence, as the only real threat or harm they ever face is the words of their anonymous detractors. Another big issue pointed out by Cain is that people reliant on social media for social interactions, must be on 24/7 as they are always waiting for that next like or text message; Cain argues this need to always been on is spurred by a combination of internet addiction and the intentional design of social media sites to mimic the addictive aesthetic of slot machines. Additionally Cain points out how FOMO or the fear of missing out, plays a big role in the negative effects of social media, as you only see what people want you to see, so it can be very easy to fall into the trap of thinking everyone’s got a better life than you. Jeff Cain is an associate professor of pharmacy practice & science at the University of Kentucky, so he is clearly quite qualified to write about this subject matter.\\n\\nHur Response: In this article Jane L. Hur and Mayank Gupta examine the effects of social media and cellphones on a variety of issues surrounding adolescents. The first issue looked at is social cognition, where they found that teens who rely on social media for their meaningful social interaction are often much more lonely than their face-to-face counterparts. Another important issue is parent-adolescent relationships, where social media has had a negative effect as most parents are woefully unequip to help their children socially navigate our digital age. An area where they found both pros and cons from social media is identity, as adolescents can join online groups that encourage personal expression, however these groups can also have harmful effects as they can promote misuse of anonymity and bad role models. Finally they found connections between social media and huge issues facing the youth of America, such as: depression, substance abuse, social anxiety, and obesity. They conclude that rather than arguing completely against adolescents using social media, parents and educators needed to be properly informed so they can help the adolescents under their care avoid the pitfalls of digital life. At the time of the article’s writing, Jane Hur was a medical student at UMDNJ-Robert Wood Johnson Medical school, however in the time since then she has finished medical school and become a full fledged MD; her co-author Mayank Gupta was a fellow in Child and Adolescent Psychiatry at the same institution when the article was written.\\n\\nLiu Response: In this article Liu introduces what he calls the augmentation and displacement hypothesis, which he feels explains the inconsistencies in research around the effects of social media on the mental health of its users. Studies have found both positive and negative effects, and Liu feels that this lack of cohesion is caused by the variety of reasons why people use media. According to Liu’s hypothesis when people use social media to augment their existing rich social interactions, it has a positive effect on their mental health as it provides them with more opportunities for meaningful interaction and the ability to meet new people. However, the flip side of this is displacement, where one uses social media for both connectedness and to escape social isolation, only furthering their isolation and mental deterioration as they become reliant on weak digital connections, that might take up time that could be used building up real face to face relationships. As part of this article Liu conducted a study on survey data from 1157 New Zealanders and through analysis of the results found his hypothesis to be supported by the data. Liu feels that when you separate people by their reasons for using social media and the quality of their face to face social interactions, the inconsistencies in research around social media start to disappear as these two factors could be at the heart of how people respond to the opportunities social media provides. Jeff Cain is extremely qualified to write on the subject of social media and its effects as he has a PhD from UCLA and is a professor of psychology at Massey University in New Zealand.\\n\\nTwenge Response: In this book Twenge looks at how cell phones and social media have shaped Americans born after 1995. She first discovered some disturbing trends around usage of cell phones, when she realized that many downward mental health trends with teens started around 2011 and 2012, right when just about everyone in America started having cellphones. She finds many detrimental effects of these forms of entertainment on the development of what see calls the iGeneration (people born after 1995). One of the most notable consequences of the iGens love for their phones, is they act younger than their own age when compared to previous generations; this has led to an entire portion of the U.S. population being developmentally behind where previous generations were at their age. This disparity along with iGeners being must more individualistic had led to a divide in attitude between the people currently running the world and their eventual replacements. Twenge feels that in order for us to properly move forward as a society we need to understand how iGen radically differs from previous generations and find a happy middle ground in which their feelings aren’t disregarded but we also try to circumvent some of their more self destructive and antisocial behaviors. Twenge is a professor of psychology at San Diego State University making her highly qualified to write about the subject of technology and youth development.', array([ 0.00826579,  0.12592942,  0.03430425, ...,  0.10913493,\n",
            "       -0.01128766,  0.10710233], dtype=float32)], ['Copy of AMS 131 Test 2 Responses.docx', 'docx', 'AMS 131\\n\\nTest 2 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n(the Exchange Paradox)\\n\\nGiven we know that the amount of money put in envelope 1 is m, then the probability of you finding m or 2m dollars in your envelope is X, so given we know m we can write the probability of X as: \\n\\nNow if we try to determine the probability of M’s value based on X we can describe that as: \\n\\nWe can use this too write the probability of getting x vs when trading, given you know X = x, as the odds between the two outcomes: \\n\\n\\n\\nGiven all this we will let Y stand for the amount of money in our opponent’s envelope, where . So we need an equation for the expected value of Y given you know X: \\n\\n\\n\\nIn this scenario the action space has two possible actions : \\n\\nWe will define the utility function as: \\n\\nWe will now define the expected value function for each possible action:\\n\\n\\n\\n\\n\\nIf then you should offer to trade, however if this isn’t true then you shouldn’t offer to trade, we will now solve the inequality: \\n\\ngiven this you should only trade if and only if: .\\n\\nDrawing of when you should and shouldn’t trade using an exponential model:\\n\\nIf your model for m is mExponential () and you see x dollars in your envelope you should offer to trade if and only if: \\n\\n. For this prior, the optimal action is to offer to trade if and only if the amount of money in your envelope is x and the amount of money in envelope 1 is E(M) and the following holds given this: .\\n\\nThe incorrect belief in paragraph 2 is that rather than calculating the proper conditional probabilities for the expected value equation, they just used for both monetary outcomes giving them an incorrect result of . However this means they did not account for the new information gained when you see how much money is in your envelope, which plays a very big role in determining the actual expected value.\\n\\n(practice with joint, marginal and conditional densities)\\n\\nThe marginal for is: \\n\\nThe marginal for is: \\n\\nThe product of these two marginals is: \\n\\nSince the product equals the joint PDF must be independent.\\n\\nThe correlation must be zero as the two variables are independent.\\n\\n\\n\\nWe will now verify that the joint density function integrates to 1: .\\n\\n\\n\\nThe marginal distribution for  is: \\n\\nThe marginal distribution for  is:\\n\\n\\n\\nChoose a fixed y2 between 0 and y1 and now integrate over y1: \\n\\n\\n\\nChoose a fixed y1 between 0 and 1 and now integrate over y2: \\n\\nFix such that ,\\n\\n\\n\\nFix such that ,\\n\\n\\n\\nGiven we already know the conditional expectations and how to calculate the variance of a random variable we just need to compute these values:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThey are dependent for two reasons, first because the support requires y2 is less than y1 and second if you multiply the two marginals distributions you don’t get the joint distribution:  \\n\\nIn order to calculate the correlation of we first need to find the standard deviation and expected value of each: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n(moment-generating functions)\\n\\nWe know that the function for the skewness of x is: , we also know that: using all this we can now better define the skewness of x as: and this will eventually equal: . The skewness will vanish when as in that case the numerator of the skew will be 0, thus making it 0. As n goes toward infinity the skewness tends toward 0, as the denominator will eventually dwarf the numerator.\\n\\n\\n\\nWe know that and the MGF of a discrete random variable is: . So given all this the MGF for the discrete random variable Y is: now we have to use the knowledge that and say that giving us the MGF: \\n\\nThe first moment is: \\n\\nThe second moment is:\\n\\n\\n\\nThe third moment is:\\n\\n\\n\\n\\n\\nThe variance of Y is: \\n\\nSince Y is a Poisson distribution its skewness is: \\n\\nThe skewness tends to zero as approaches .\\n\\n\\n\\n(archaeology)\\n\\nSince the model is uniform and IID, the marginal distribution for each , because of this as the multiplying of all the indicators will only be 1 if the max y value is less than , so in essence this is the only thing that matters for evaluating if the indicator will return true when determining the joint distribution. Since the model is uniform and IID its joint distribution is just the product of all marginals:  \\n\\n\\n\\n\\n\\nBased on this graph the MLE for this problem must be:  as the likelihood function has its greatest value at m.\\n\\nThe function is discontinuous at its maximum so it can’t be differentiated to find the maximum value.\\n\\n\\n\\nIt is normalized if you use the c in the likelihood function to ensure it integrates to 1, doing this allows you to model this function using a more standard PDF. Given this we can model the likelihood as the pareto distribution with the parameters , as when  the indicator function is the same for both when you treatas w, additionally we can replace with c as this is just for normalization, and finally we can make when .\\n\\nIf we take both the prior distribution (parameters: ) and the likelihood function (parameters: ) as pareto distributions, then the resulting posterior distribution is just another pareto distribution as any two pareto distributions multiplied together is another pareto distribution. The math showing this can be found below: .\\n\\nTherefore our resulting posterior distribution is a pareto distribution with parameters: .\\n\\n\\n\\nIn the provided data set m = 5.1 and n = 11. We are also given that using this we get the below graph:\\n\\nGiven our values provided in (i) we can now calculate the posterior mean and SD below: \\n\\nPosterior variance: given this the posterior SD is:  and the mean is: . We can now rewrite the sentence from the problem: On the basis of this prior and data information, the value for this species of fossil ammonite is about , give or take about .', array([ 0.00826579,  0.136938  ,  0.04765578, ...,  0.01271085,\n",
            "       -0.00748783,  0.05940653], dtype=float32)], ['Copy of Writing 2 #10 Rhetorical Analysis on Kasser(1).docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nKasser Rhetorical Analysis\\n\\nAuthor’s Purpose: Kasser seems to have written this piece for two reasons, first too expose the connections between common psychological issues and materialism and second to argue contemporary therapy techniques should use these conclusions when attempting to diagnosis patients. The majority of the piece focuses on the first purpose, however in the final portion of the piece he makes a compelling case for his second purpose.\\n\\nAudience: This piece is probably directed at academics, medical professionals, and people interested in the effects of materialism. While he does make an appeal to medical professionals at the end of the piece, it still seems like something that anyone could pick up and getting something out of.\\n\\nAuthor’s Credibility: He has a PhD in psychology from the University of Rochester, he is a professor of psychology at Knox College, and he has written more than 100 scientific articles on materialism. He seems to be pretty well versed in the piece’s subject matter.\\n\\nEmotional, ethical, or human connections: Kasser does not make many intentional emotional appeals, however since many of his points hit home for most readers, there is still a component that you can connect with on a human level.\\n\\nLogic/Evidence: Kasser does a great job of presenting evidence from multiple experts in order to support and complement his arguments and analysis. He takes a very logical approach to how he draws his conclusions, making it easy for the reader to understand how he went from the raw data to his greater arguments around human psychology. Of all the pieces we have read I found this one to have the most compelling evidence and logic, as Kasser’s years of experience in the field have properly equipped him to find the best evidence possible.\\n\\nOrganization: This was a very methodically organized essay. It begins with a basic overview of materialism in our society and how most people would explain away its utility, then it moves into how people become materialistic and the detrimental effects it has, and finally it concludes on a call to incorporate this work into how psychological orders are understood and treated.\\n\\nStyle/Tone: This essay has a pretty serious and straight forward time. Kasser doesn’t really waste anytime with flowery language or emotional appeals, this essay has almost no fat, just evidence and arguments.\\n\\nGenre: I would probably classify this as an academic article, as it draws heavily on academic sources and talks directly to medical professionals in its conclusion.\\n\\nOverall: I really enjoyed this piece, many of the arguments Kasser makes are things that have long floated around in the back of my head, but I couldn’t put my finger on until I read this. I honestly plan on trying to use some of what he says to reflect on my own life and maybe try and make a few adjustments to be a healthier, happier person. I will say however, that I was bothered on pg.14 when he is describing the effects of family environments and argues that both lax and strict parenting both lead to people becoming materialistic; this bothers me because if both ways lead to materialism, then what doesn’t? Like most parents usually fall on one side of the binary, I have never encountered a parent who’s perfectly in the middle. How should one best approach raising a child when the two most logical approaches are guaranteed to screw your child over. As a scientifically minded person this especially irritates me as if you have a conditional event, but both possibilities lead to the same outcome, then the outcome is not conditional on which of the two events occur (i.e. in this case there would be no correlation between parenting and materialism, because both primary paths of parenting lead to materialism).', array([0.01668783, 0.14270696, 0.03430425, ..., 0.09834626, 0.07130326,\n",
            "       0.09691838], dtype=float32)], ['Copy of Writing 2 #15 Annotated Bibliography.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nAnnotated Bibliography\\n\\nCain, Jeff. “It’s Time to Confront Student Mental Health Issues Associated with Smartphones and Social\\n\\nMedia.” American Journal of Pharmaceutical Education 82.7 (2018): 738–741. Web.\\n\\nIn this article Cain looks at how the emerging mental health crisis among college undergraduate and graduate students is connected to social media use. The first big takeaway of the piece comes from another medical academic named Eiser who argues that social media dulls people’s ability to properly handle conflict by allowing the creation of intellectual safe spaces. This then has a compounding effect with our current political turmoil to create a situation where users are surrounded by affirmation when they stay within their circles and are met by opposition when they leave said circles; this promotes anti-social behaviour as it makes people not want to leave their digital groups. It also causes people to falsely equate words with violence, as the only real threat or harm they ever face is the words of their anonymous detractors. Another big issue pointed out by Cain is that people reliant on social media for social interactions, must be on 24/7 as they are always waiting for that next like or text message; Cain argues this need to always been on is spurred by a combination of internet addiction and the intentional design of social media sites to mimic the addictive aesthetic of slot machines. Additionally Cain points out how FOMO or the fear of missing out, plays a big role in the negative effects of social media, as you only see what people want you to see, so it can be very easy to fall into the trap of thinking everyone’s got a better life than you. Jeff Cain is an associate professor of pharmacy practice & science at the University of Kentucky, so he is clearly quite qualified to write about this subject matter.\\n\\n\\n\\nHur, Jane & Gupta, Mayank. (2013). Growing up in the Web of Social Networking: Adolescent\\n\\nDevelopment and Social Media. Adolescent psychiatry. 3. 10.2174/2210676611303030004.\\n\\nIn this article Jane L. Hur and Mayank Gupta examine the effects of social media and cellphones on a variety of issues surrounding adolescents. The first issue looked at is social cognition, where they found that teens who rely on social media for their meaningful social interaction are often much more lonely than their face-to-face counterparts. Another important issue is parent-adolescent relationships, where social media has had a negative effect as most parents are woefully unequip to help their children socially navigate our digital age. An area where they found both pros and cons from social media is identity, as adolescents can join online groups that encourage personal expression, however these groups can also have harmful effects as they can promote misuse of anonymity and bad role models. Finally they found connections between social media and huge issues facing the youth of America, such as: depression, substance abuse, social anxiety, and obesity. They conclude that rather than arguing completely against adolescents using social media, parents and educators needed to be properly informed so they can help the adolescents under their care avoid the pitfalls of digital life. At the time of the article’s writing, Jane Hur was a medical student at UMDNJ-Robert Wood Johnson Medical school, however in the time since then she has finished medical school and become a full fledged MD; her co-author Mayank Gupta was a fellow in Child and Adolescent Psychiatry at the same institution when the article was written.\\n\\n\\n\\nGlaser, Philip et al. “Is Social Media Use for Networking Positive or Negative? Offline Social Capital and\\n\\nInternet Addiction as Mediators for the Relationship Between Social Media Use and Mental\\n\\nHealth.” New Zealand Journal of Psychology (Online) 47.3 (2018): 12–18. Web.\\n\\nIn this article Liu introduces what he calls the augmentation and displacement hypothesis, which he feels explains the inconsistencies in research around the effects of social media on the mental health of its users. Studies have found both positive and negative effects, and Liu feels that this lack of cohesion is caused by the variety of reasons why people use media. According to Liu’s hypothesis when people use social media to augment their existing rich social interactions, it has a positive effect on their mental health as it provides them with more opportunities for meaningful interaction and the ability to meet new people. However, the flip side of this is displacement, where one uses social media for both connectedness and to escape social isolation, only furthering their isolation and mental deterioration as they become reliant on weak digital connections, that might take up time that could be used building up real face to face relationships. As part of this article Liu conducted a study on survey data from 1157 New Zealanders and through analysis of the results found his hypothesis to be supported by the data. Liu feels that when you separate people by their reasons for using social media and the quality of their face to face social interactions, the inconsistencies in research around social media start to disappear as these two factors could be at the heart of how people respond to the opportunities social media provides. Jeff Cain is extremely qualified to write on the subject of social media and its effects as he has a PhD from UCLA and is a professor of psychology at Massey University in New Zealand.\\n\\n\\n\\nTwenge, Jean M. IGen\\u202f: Why Today’s Super-Connected Kids Are Growing up Less Rebellious, More\\n\\nTolerant, Less Happy-- and Completely Unprepared for Adulthood (and What This Means for the\\n\\nRest of Us) . First Atria books hardcover edition. New York, NY: Atria Books, 2017. Print.\\n\\nIn this book Twenge looks at how cell phones and social media have shaped Americans born after 1995. She first discovered some disturbing trends around usage of cell phones, when she realized that many downward mental health trends with teens started around 2011 and 2012, right when just about everyone in America started having cellphones. She finds many detrimental effects of these forms of entertainment on the development of what see calls the iGeneration (people born after 1995). One of the most notable consequences of the iGens love for their phones, is they act younger than their own age when compared to previous generations; this has led to an entire portion of the U.S. population being developmentally behind where previous generations were at their age. This disparity along with iGeners being must more individualistic had led to a divide in attitude between the people currently running the world and their eventual replacements. Twenge feels that in order for us to properly move forward as a society we need to understand how iGen radically differs from previous generations and find a happy middle ground in which their feelings aren’t disregarded but we also try to circumvent some of their more self destructive and antisocial behaviors. Twenge is a professor of psychology at San Diego State University making her highly qualified to write about the subject of technology and youth development.', array([ 0.00826579,  0.12592942,  0.03430425, ...,  0.07621301,\n",
            "       -0.02593233,  0.1064028 ], dtype=float32)], ['Copy of Copy of AMS 131 Test 2 Responses.docx', 'docx', 'AMS 131\\n\\nTest 2 Responses\\n\\nMax Alfano-Smith (ID 1684138)\\n\\n\\n\\n(the Exchange Paradox)\\n\\nGiven we know that the amount of money put in envelope 1 is m, then the probability of you finding m or 2m dollars in your envelope is X, so given we know m we can write the probability of X as: \\n\\nNow if we try to determine the probability of M’s value based on X we can describe that as: \\n\\nWe can use this too write the probability of getting x vs when trading as the odds between the two outcomes: \\n\\n\\n\\nGiven all this we will let Y stand for the amount of money in our opponent’s envelope. So we need an equation for the expected value of Y given you know X: \\n\\nS\\n\\ns\\n\\n(practice with joint, marginal and conditional densities)\\n\\nS\\n\\nS\\n\\nS\\n\\nS\\n\\nThe marginal distribution for y1 is: \\n\\nThe marginal distribution for y2 is:\\n\\ns\\n\\nThey are dependent for two reasons, first because the support requires y2 is less than y1 and second if you multiply the two marginals distributions you don’t get the joint distribution:  \\n\\ns\\n\\n(moment-generating functions)\\n\\nD\\n\\nK\\n\\nWe know that and the MGF of a discrete random variable is: . So given all this the MGF for the discrete random variable Y is: now we have to use the knowledge that and say that giving us: \\n\\n[NOTE] skewness(Y) = \\n\\nD\\n\\nD\\n\\nd\\n\\n(archaeology)\\n\\nD\\n\\nD\\n\\nd', array([ 0.00826579,  0.12118347,  0.04418642, ...,  0.01271085,\n",
            "       -0.00748783,  0.05940653], dtype=float32)], ['Copy of Writing 2 #10 Rhetorical Analysis on Kasser.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nKasser Rhetorical Analysis\\n\\nAuthor’s Purpose: Kasser seems to have written this piece for two reasons, first too expose the connections between common psychological issues and materialism and second to argue contemporary therapy techniques should use these conclusions when attempting to diagnosis patients. The majority of the piece focuses on the first purpose, however in the final portion of the piece he makes a compelling case for his second purpose.\\n\\nAudience: This piece is probably directed at academics, medical professionals, and people interested in the effects of materialism. While he does make an appeal to medical professionals at the end of the piece, it still seems like something that anyone could pick up and getting something out of.\\n\\nAuthor’s Credibility: He has a PhD in psychology from the University of Rochester, he is a professor of psychology at Knox College, and he has written more than 100 scientific articles on materialism. He seems to be pretty well versed in the piece’s subject matter.\\n\\nEmotional, ethical, or human connections: Kasser does not make many intentional emotional appeals, however since many of his points hit home for most readers, there is still a component that you can connect with on a human level.\\n\\nLogic/Evidence: Kasser does a great job of presenting evidence from multiple experts in order to support and complement his arguments and analysis. He takes a very logical approach to how he draws his conclusions, making it easy for the reader to understand how he went from the raw data to his greater arguments around human psychology. Of all the pieces we have read I found this one to have the most compelling evidence and logic, as Kasser’s years of experience in the field have properly equipped him to find the best evidence possible.\\n\\nOrganization: This was a very methodically organized essay. It begins with a basic overview of materialism in our society and how most people would explain away its utility, then it moves into how people become materialistic and the detrimental effects it has, and finally it concludes on a call to incorporate this work into how psychological orders are understood and treated.\\n\\nStyle/Tone: This essay has a pretty serious and straight forward time. Kasser doesn’t really waste anytime with flowery language or emotional appeals, this essay has almost no fat, just evidence and arguments.\\n\\nGenre: I would probably classify this as an academic article, as it draws heavily on academic sources and talks directly to medical professionals in its conclusion.\\n\\nOverall: I really enjoyed this piece, many of the arguments Kasser makes are things that have long floated around in the back of my head, but I couldn’t put my finger on until I read this. I honestly plan on trying to use some of what he says to reflect on my own life and maybe try and make a few adjustments to be a healthier, happier person. I will say however, that I was bothered on pg.14 when he is describing the effects of family environments and argues that both lax and strict parenting both lead to people becoming materialistic; this bothers me because if both ways lead to materialism, then what doesn’t? Like most parents usually fall on one side of the binary, I have never encountered a parent who’s perfectly in the middle. How should one best approach raising a child when the two most logical approaches are guaranteed to screw your child over. As a scientifically minded person this especially irritates me as if you have a conditional event, but both possibilities lead to the same outcome, then the outcome is not conditional on which of the two events occur (i.e. in this case there would be no correlation between parenting and materialism, because both primary paths of parenting lead to materialism).', array([0.01668783, 0.130511  , 0.03430425, ..., 0.10843325, 0.05482299,\n",
            "       0.09857009], dtype=float32)], ['Copy of Writing 2 #19 Metacognition Reflection on Group Presentation.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nMetacognition Reflection on Group Presentation\\n\\nWe made our project using google slides. We decided to use this presentation medium because it is the best way to provide concise talking points and engaging imagery. Our presentation is about the importance of being outside so it was essential we could show off the beauty of nature and being in it.\\n\\nThe main idea that we wanted to convey was that it is super important to get outside. It is so easy when your constantly working to never spend time outside, but in the long run this is very bad for you. I know from my own personal experience that I am a much happier and productive person when I go for a few hikes a week.\\n\\nAs I said in question two we felt that it was very important to incorporate visual elements into every slide, as a big part of being in and appreciating nature is just observing it. By showing our classmates a small glimpse of the beauty the outdoors provide, it will hopefully motivate them to spend some time outside in the last few days of the quarter. Additionally having a lot of visuals is just a good presentation strategy in general as most people get bored when they are not engaged visually.\\n\\nI helped come up with the idea for the presentation and created the two slides that I presented. I was initially worried about the group nature of this project. But once we actually sat down and just worked at it, I found we actually made a pretty good team.\\n\\nIt definitely made me realize how important the outdoors have become to me. Ever since I came to UCSC my appreciation for nature has grown exponentially and this project showed me the heights it has reached after a year here.', array([0.00826579, 0.15231937, 0.03430425, ..., 0.03139563, 0.00861819,\n",
            "       0.0980081 ], dtype=float32)], ['Copy of Writing 2 #9 Rhetorical Analysis on Putnam(1).docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nPutnam Rhetorical Analysis\\n\\nAuthor’s Purpose: Putnam sets out to argue that social connections are one of, if not the most important factor in personal health. He wants the reader to understand that in order to be happy and healthy one needs to maintain close relationships, both platonic and romantic. Putnam feels that right now there is as much evidence for the benefits of social connections as there was for the negative effects of cigarettes when the surgeon general’s warning first appeared on them.\\n\\nAudience: Putnam is most likely talking to people who want to live happy and healthy lives. I personally find this article to be a bit of a duh moment, but I bet there are people out there who have yet to realize how important social connections are to your personal wellbeing.\\n\\nAuthor’s Credibility: He is a public policy professor at Harvard University so he probably knows a good people about how people interact and public health.\\n\\nEmotional, ethical, or human connections: For a paper all about social connections, Putnam actually does not use a lot of anecdotes or stories to convince the reader. I think this is a noble approach as given the genre it is extremely easy to just get by on emotional appeals, rather than real evidence and solid arguments.\\n\\nLogic/Evidence: These are Putnam’s primary two tools for making his argument, he is constantly citing both expert opinions and statistics in order to properly support his case. I really enjoyed the highly logical lens he applied to his evidence, as I have come to many of the same conclusions as him through similar means of deduction in my life.\\n\\nOrganization: Putnam structures this piece in a pretty linear manner, making his argument, supporting it with evidence, and then addressing possible counterarguments or conflating factors.\\n\\nStyle/Tone: This is a pretty serious and to the point piece. He doesn’t waste any time with personal appeals and instead focuses on hard facts and reason. It would not have surprised me if this article was a written by an actual scientist given how effectively he utilizes hard data.\\n\\nGenre: I would probably classify this as an argumentative research paper given how much data he references, while still trying to make a point.\\n\\nOverall: I definitely enjoyed this piece as it confirmed a lot of my own opinions around social connections and our wellbeing. However, I will say most of these papers have felt very similar and in my mind this may be the biggest offender as its primary argument, social connection is essential to our physical and emotional wellbeing, is something I have picked up on in every single other paper we have read; making this one feel like a bit of a been there done that experience.', array([0.01586739, 0.15981683, 0.03430425, ..., 0.06182393, 0.05067324,\n",
            "       0.0952611 ], dtype=float32)], ['SNS1.docx', 'docx', 'Meeting Minutes\\n\\nFaceTime meeting 8AM 11/5\\n\\n\\n\\nAttendance: Dena, Katya, Max, Ashley, Hector, Max\\n\\n\\n\\nPLAN: Reviewing interviews, coming up with new questions, constructing graph of key insights\\n\\n\\n\\nDiscussion\\n\\nInterview Recaps:\\n\\nAshley: 2 interviews—> like to plan the night before, athletic style, but not apathetic about style despite how it’s just athletic, like to shop online, cheaper brands, sales, want to look good but won’t make huge financial sacrifices—> saving money & expression of the self\\n\\nKatya: brands don’t really matter; it just has to look cool (this is good because usually other people’s clothes you trade won’t be super ‘top notch’); uniqueness is super important; ‘one of a kind’--> status, expression of the self\\n\\nHector: SHOES! —> shops for shoes on Depop, buys hype shoes online, refrains from purchasing in-store; “sneakerhead”; reasoning: uniqueness&affordability—> expression of the self \\n\\nThrift store managers can come into play here in terms of having their respective SwipeNSwap accounts\\n\\nMaybe one of us can go to a respective thrift store and talk to managers there for key insights\\n\\nDena: priority on being comfortable and looking unique while at the same time preserving affordability\\n\\nMax: sick, interviews pending! Plans to make them up by this weekend. Plans to interview freshmen in his building potentially.\\n\\n\\n\\nReviewing & Constructing Hypothesis: Trading clothes via a user-friendly app can yield both social and intrapersonal (expression) gains for fashion-oriented consumers seeking both variety and cost-effectiveness.\\n\\n\\n\\nNEW QUESTIONS TO ADD\\n\\nWhat makes you feel your best while you’re shopping or after you’re done shopping?\\n\\nIf you could change one element of your shopping experience, what would it be?\\n\\nWhat’s your preferred interface in terms of shopping?\\n\\nAt what point do you get rid of/recycle your clothing?\\n\\nWhat do you do with clothing you no longer wear/are invested in?\\n\\nRevenue Streams\\n\\n\\n\\nPremium memberships\\n\\nUnlimited right swipes\\n\\nCan see who swiped right on your clothes\\n\\nChat with anybody, regardless of if you matched with them (like + send a message) ‘super-like’\\n\\nYou can go back on something you accidentally swiped left on\\n\\n“Boost” algorithm to hype up your account\\n\\nMax doesn’t think stylists are viable; element of partnering with stylists is pretty antithetical to the simple mechanism of the app (niche, humble)--> ask for advice?\\n\\nAlso stylists will likely be premium members anyway\\n\\nThey’ll use it regardless, doesn’t have to be a whole separate element within the app\\n\\nPerhaps thrift stores and brands can have accounts—> people can sell or trade clothes? \\n\\nLocal businesses—> advertising us, could be like a trade (e.g., CookieCruz)\\n\\n\\n\\nAction Items:\\n\\n\\t-pursue 2-3 more interviews for the next week while incorporating the new questions\\n\\n\\t-somehow find a mentor, get help with this\\n\\n\\t-record in detail key insights \\n\\n\\t-maintain good communication with group\\n\\n\\t-potentially reach out to local business\\n\\nNext Steps: Meet this weekend (Sunday at 5pm)', array([-0.01049833, -0.08939896, -0.03044511, ..., -0.03926747,\n",
            "       -0.03814263, -0.02892261], dtype=float32)], ['Copy of Writing 2 First Essay Second Draft.docx', 'docx', 'Max Alfano-Smith\\n\\nSandy Archimedes\\n\\nApril 27, 2019\\n\\nEssay #1, Draft #2\\n\\nLive for now\\n\\nAll we have is the moment. While we might be able to think and talk about the past and future, the only space we can truly inhabit is right now. When we forget or try to ignore this fact of life, we can easily lead ourselves astray. Far too often we look back on our past with regret and toward our future wishing we could control it. But we are only people, we will never be able to change our past or preordain our future; our agency is perpetually trapped in the moment. Coming to terms with this limitation is one of the hardest aspects of life and the proverbial end boss of most people’s quest for happiness. But why is it so hard for us to merely live in the moment? Especially when it is so critical to our mental and emotional wellbeing? We face this struggle, because our ability to plan for the future and learn from the past has carried our species further than just about anything else. Our mind is at constant odds with itself, on the one hand we want to live in the moment, while on the other we feel we must constantly reflect on our past and plan for the future. Finding balance between these two aspects of ourselves is the key to both living in the moment and being mindful of the past and future.\\n\\n            Our obsession with our past and future has led to a constant anxiety around what we should have done differently and what’s to come next. We struggle to draw satisfaction from our accomplishments because we often come to expect and account for them, long before the actual moment of triumph. In his essay, “A Path of Radical Sobriety,” Daizui MacPhillamy perfectly sums up the self-destructive nature of this obsession as, “If we’re never satisfied, it guarantees that we can never really be completely at peace within ourselves and also that we never get a chance to fully enjoy the simple pleasures that make life worthwhile” (99). As MacPhillamy points out our lack of satisfaction has forced us to lead lives in which our idea of happiness is more of a light at the end of the tunnel, instead of an actual state of being. MacPhillamy feels that the root of our dissatisfaction and inability to enjoy the moment is the fact we attach an expectation of happiness to upcoming events in our life. We convince ourselves that in order to be happy we need to accomplish some taxing objective and use this as the motivation to achieve our goal (104).Unfortunately, this expectation of happiness prevents one from enjoying their labor as it’s merely a means to an end and, once one does achieve their goal, their enjoyment is overshadowed by thoughts of how they can go even further.\\n\\nI have experienced this vicious cycle of expectation many times in my life; however, I have found it most prevalent around activities in which the possibility of expanding one’s monetary wealth is involved. I think the very concept of money plays right into our constant need for something better, as it is the easiest way we have found to designate something’s worth relative to all other things. Recognizing the fact money goes hand in hand with dissatisfaction is something that took me years to realize, but brought me much clarity on my own priorities and desires. If one spends their entire life thinking about money, they have no true aspirations, just a desire for some undefined notion of material success; it’s the undefined nature of this success that I think makes it so corrosive, because if all one wants is more material, they will never be satisfied by any singular acquisition, trapping them in a perpetual cycle of desire.\\n\\n            Dissatisfaction most often stems from unmet expectations, so in order to truly understand what makes us dissatisfied we must exam the genesis of these expectations. In Rick Hanson’s book Resilient, we are offered an explanation of how our unhealthy expectations come to be. Hanson sees our notions of “liking” something and “wanting” something as separate entities that don’t imply one another. To exemplify someone who wants but does not like, Hanson describes those he has seen wasting their lives away at slot machines, dispassionately pulling levers in hopes of winning big (156). Hanson describes the relationship between “liking” and “wanting” as, “liking without wanting is heaven, but wanting without liking is hell” (156, 157).This saying is very revealing about the negative nature of desire, as it does not say heaven is balance between the two, but instead it is the complete absence of wanting.\\n\\nEven if you do like something, wanting it is still an uncomfortable state, as you are always on edge, wondering when you’ll get your next fix of whatever it is you desire. It is extremely easy to convince yourself that this desire is a good thing, especially if that which you desire is considered a positive in the eyes of your peers or society. However, one must fight this desire to rationalize their wants, as before they know it, their satisfaction will have dried up and they’ll be left with a need for something they don’t enjoy. It’s this need for things that we don’t like or are indifferent too that is at the heart of our unhealthy expectations. It drives us to put our energy into things that only make us discontent, as we find pleasure in neither the ends nor the means.\\n\\n            Unfortunately becoming present in the moment by freeing oneself of unhealthy expectations is much easier said than done; it is something I struggled with for many years. I am an avid recreational programmer and trying to keep your expectations in check around how well your app will do is almost impossible when you live in Silicon Valley and it seems like everyone’s got an app, they’re getting rich off of. The best way I have found to combat expectations is to be aware of them and ensure that they grow slowly. Don’t instantly expect you’ll get what you want, even if all the signs point that way. Hold out on savoring a moment until it actually comes. Even if an event eventually comes to pass in one’s favor, the worst thing one can do is prematurely celebrate. By celebrating prematurely, one exhausts the happiness success brings, crippling their ability to enjoy the actual moment. While I am not advocating pessimism, I often to try to anticipate events not going my way and coming to terms with that, rather than assuming I’ll get what I want. By ensuring your expectations are grounded in reality, rather than an idealized one, you can prevent the sadness that comes from dashed hopes. Doing this also helps one stay in the moment, because nothing pulls one out of the real world and into their head quicker than desires gone awry. While our ability to constantly want more has brought us far, if one wants to truly be happy, they must get ahold of those wants, before they get ahold of them.\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nWorks Cited:\\n\\nHanson, Rick. Resilient: How to Grow an Unshakable Core of Calm, Strength, and Happiness.\\n\\nWriting 2 Reader. Ed. Archimedes. UCSC Professor Publishing, 2019.\\n\\nMacPhillamy, Daizui. “A Path of Radical Sobriety”. Writing 2 Reader. Ed. Archimedes. UCSC\\n\\nProfessor Publishing, 2019.\\n\\n \\n\\n\\n\\nAlfano-Smith \\n\\nAlfano-Smith', array([ 0.00826579,  0.12592942,  0.03430425, ..., -0.0001995 ,\n",
            "       -0.00563768,  0.09550191], dtype=float32)], ['Copy of Writing 2 #9 Rhetorical Analysis on Putnam.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nPutnam Rhetorical Analysis\\n\\nAuthor’s Purpose: Putnam sets out to argue that social connections are one of, if not the most important factor in personal health. He wants the reader to understand that in order to be happy and healthy one needs to maintain close relationships, both platonic and romantic. Putnam feels that right now there is as much evidence for the benefits of social connections as there was for the negative effects of cigarettes when the surgeon general’s warning first appeared on them.\\n\\nAudience: Putnam is most likely talking to people who want to live happy and healthy lives. I personally find this article to be a bit of a duh moment, but I bet there are people out there who have yet to realize how important social connections are to your personal wellbeing.\\n\\nAuthor’s Credibility: He is a public policy professor at Harvard University so he probably knows a good people about how people interact and public health.\\n\\nEmotional, ethical, or human connections: For a paper all about social connections, Putnam actually does not use a lot of anecdotes or stories to convince the reader. I think this is a noble approach as given the genre it is extremely easy to just get by on emotional appeals, rather than real evidence and solid arguments.\\n\\nLogic/Evidence: These are Putnam’s primary two tools for making his argument, he is constantly citing both expert opinions and statistics in order to properly support his case. I really enjoyed the highly logical lens he applied to his evidence, as I have come to many of the same conclusions as him through similar means of deduction in my life.\\n\\nOrganization: Putnam structures this piece in a pretty linear manner, making his argument, supporting it with evidence, and then addressing possible counterarguments or conflating factors.\\n\\nStyle/Tone: This is a pretty serious and to the point piece. He doesn’t waste any time with personal appeals and instead focuses on hard facts and reason. It would not have surprised me if this article was a written by an actual scientist given how effectively he utilizes hard data.\\n\\nGenre: I would probably classify this as an argumentative research paper given how much data he references, while still trying to make a point.\\n\\nOverall: I definitely enjoyed this piece as it confirmed a lot of my own opinions around social connections and our wellbeing. However, I will say most of these papers have felt very similar and in my mind this may be the biggest offender as its primary argument, social connection is essential to our physical and emotional wellbeing, is something I have picked up on in every single other paper we have read; making this one feel like a bit of a been there done that experience.', array([0.01586739, 0.15981683, 0.03430425, ..., 0.06664109, 0.05482299,\n",
            "       0.10105146], dtype=float32)], ['Copy of Writing 2 #8B Metacognition Reflection on Essay #1.docx', 'docx', 'Writing 2\\n\\nSandy Archimedes\\n\\nMax Alfano-Smith\\n\\nMetacognition Reflection on Essay #1\\n\\nWhile writing this essay I definitely struggled with the exact genre it falls into. I have written research and argumentative essays, however this essay seemed to fall into a weird gray area between the two, because there’s an expert source component but your also making an argument with a thesis statement. In the end I think I found a good balance but when I first started the essay I was a bit uneasy on how much of each component would work best.\\n\\nI split up my two body paragraphs into four total and added a bit to make each one work as a standalone paragraph. I felt this was the right thing to do as it split up my expert evidence from my own anecdotes, which I think makes it easier for the reader to understand when I am talking and when I am referencing someone else.\\n\\nThis essay definitely taught me about how to incorporate ideas from my homework and class discussions into my writing. Usually when I write essays I end up writing something pretty unrelated to the ideas I have discussed in class, but in this instance I found the ideas from our class discussions interesting enough to write about. This actually made the writing process much easier as I already had a real good grip of what I wanted to say.\\n\\nI think my analysis is the strongest part of my essay. I felt it was extremely important to fully and accurately convey my opinions on this topic as its something I care a great deal about.\\n\\nI think I could have done a better job integrating more expert sources, but while writing the essay I really struggled with going back to the sources to find evidence that perfectly supported my arguments.\\n\\nI initially underestimated how long this paper would take me, so my rough draft was a bit more unpolished than I would have liked, but I think in the week I’ve had since then to revise, it has gotten pretty good.', array([0.00826579, 0.12592942, 0.03430425, ..., 0.01966578, 0.03137136,\n",
            "       0.09518265], dtype=float32)], ['SNS2.docx', 'docx', 'Meeting Minutes\\n\\nFaceTime meeting 10PM 11/11\\n\\n\\n\\nAttendance: Dena, Katya, Max, Ashley, Hector, Max\\n\\n\\n\\nPLAN: Reviewing interviews, coming up with abbreviated presentation, discover our new insights & future directions\\n\\n\\n\\nDiscussion\\n\\nInterview Recaps:\\n\\nAshley: Huge selection in stores, seeking some sort of narrowness in terms of honing in on their own personal style. What makes you feel best when you shop? —> saving TIME while preserving their own unique style. Online shopping is  a huge element of this.; PJs: if it’s worn out or old they will stop wearing it. Donate to goodwill. —> saving money & expression of the self\\n\\nKatya: Usually never buy anything online, but will spend a lot of time searching online to prepare them for going into the store; wear clothing until they get “bored” of it; donate to goodwill.--> status, expression of the self\\n\\nHector: interviewed his coworker. When it comes to trading clothes, they usually wear it 2 or 3 times and then resell it before it gets too worn out; someone brought up the idea of maybe doing a buddy system—> matching with other buddies on the app. Might feel safer for people; less intimidating. \\n\\nDena: Feeling the best when clothes represent their truest aesthetics/color combinations/’hype’ style; when they can be affordable; when time doesn’t have to be spent on attaining the most aesthetic clothing. Preferred interface: online; wish there was more specific sizing options though; DONATING/giving them to friends/yard sales; wearing clothes either until they are out of season/style or until there is bad damage to them (unwearable).\\n\\nMax: interviewed 4 people. 2 of them very very apathetic about style, “grab whatever they can find,” (CS majors excluded from our market)--> big on online shopping, 3⁄4 would almost always make purchases online, don’t engage in shopping too much aside from being back at home (away from college); young professional insight: almost all your $ on clothes is spent on clothes for your job… perhaps we could somehow angle the professionalism element— maybe create filters (outerwear, athletic, dressy, professional, casual,etc.); everyone said they wear clothes until it’s unusable; left it for parents to deal with when they’re done with them, mom recycles clothing—> makes rags \\n\\nHypothesis: Trading clothes via a user-friendly app can yield both social and intrapersonal (expression) gains for fashion-oriented consumers seeking both variety and cost-effectiveness.\\n\\n\\n\\nQUESTIONS TO CONTINUE INTERVIEWING WITH\\n\\nWhat makes you feel your best while you’re shopping or after you’re done shopping?\\n\\nIf you could change one element of your shopping experience, what would it be?\\n\\nWhat’s your preferred interface in terms of shopping?\\n\\nAt what point do you get rid of/recycle your clothing?\\n\\nWhat do you do with clothing you no longer wear/are invested in?\\n\\n\\n\\n—> maybe we should ask a specific question: do you ever trade clothes with people; when was the last time you traded clothes? \\n\\nA couple of us can go to thrift / small antique stores downtown and elevator pitch our idea; see if they’d be down.\\n\\nRevenue Streams\\n\\n\\n\\nPremium memberships\\n\\nUnlimited right swipes\\n\\nCan see who swiped right on your clothes\\n\\nChat with anybody, regardless of if you matched with them (like + send a message) ‘super-like’\\n\\nYou can go back on something you accidentally swiped left on\\n\\n“Boost” algorithm to hype up your account\\n\\nMax doesn’t think stylists are viable; element of partnering with stylists is pretty antithetical to the simple mechanism of the app (niche, humble)--> ask for advice?\\n\\nAlso stylists will likely be premium members anyway\\n\\nThey’ll use it regardless, doesn’t have to be a whole separate element within the app\\n\\nPerhaps thrift stores and brands can have accounts—> people can sell or trade clothes? \\n\\nLocal businesses—> advertising us, could be like a trade (e.g., CookieCruz)\\n\\n\\n\\nAction Items:\\n\\n\\t-pursue 2-3 more interviews for the next week while incorporating the new \\n\\nquestions, maybe going to small stores/thrift stores\\n\\n\\t-Meet with Deborah eventually\\n\\n\\t-record in detail key insights \\n\\n\\t-maintain good communication with group\\n\\n\\t-potentially reach out to local business\\n\\n\\t-as far as returns go, we are 0 liability\\n\\nNext Steps: Meet this weekend (Sunday at 5pm)', array([-0.01049833, -0.08939896, -0.03044511, ..., -0.03926747,\n",
            "       -0.03814263, -0.02892261], dtype=float32)], ['SNS3.docx', 'docx', 'Meeting Minutes\\n\\nFaceTime meeting 9AM 11/17\\n\\n\\n\\nAttendance: Deborah, Dena, Katya, Max, Ashley, Hector, Max\\n\\n\\n\\nPLAN: Mentorship advice and future directions\\n\\n\\n\\nDiscussion\\n\\n\\n\\nDeborah-- sustainable fashion industry\\n\\nVery well versed on developing an effective pitch\\n\\nAdvice: take a business class regardless of your path/general major\\n\\nSustainable fashion industry is beginning to boom!--> internationally it’s a really big topic (Ellen MacArthur Foundation-- circular fashion-- exactly what SwipeN’Swap essentially is)--> supply chain (seeds in the ground→ commercialization);usually goes from cradle to grave→ comes out of the ground and goes back into the ground→ BROKEN SYSTEM→ messes with landfills, oceans. \\n\\nDeborah comes in at the end of use and asks us how can our items be circulated?\\n\\nShifting: cradle to cradle; keep finding ways to reintroduce something to the world until it finally has no use anymore, so that it doesn’t break down into anything toxic when it goes into the ground\\n\\n“Fashion for Good”--> many high fashion companies have started to take clothes back→ general rise in interest of secondhand items\\n\\nLook under “ThreadUp”--> company that makes an annual report on the amount of annual used clothing → great statistics for our pitch!--> why is this important now; what’s going on with the market?\\n\\nSellHound→ desktop version → marketplace directory→ SilkRoll; SwopIt\\n\\nSwipeN’Swap is a trading platform and it is a marketplace\\n\\nElevator pitch: Problem (HAVE ONE PROBLEM THOUGH! fashion is too expensive, college students=broke, we all have too much clothing-- fashion is a super dirty industry, get stats, clogging up landfills, college students don’t want to spend money), Value Proposition (the app is local, easy, convenient, user interface--> why are you better than the rest?), Business Solution,\\n\\nMake a persona of who the person is that’d be using your app-- demographics→ provides a marketing model\\n\\nElevator Pitch: how to say the problem→ hi, we’re team A, we’ve developed this app, swipenswap, what drove us to develop this is XYZ problem, and here is the solution, and the reason we think it’s really great is because of XYZ value proposition; rethink the word ‘free’; our team is all a part of the demographic\\n\\nHi, we are XYZ from SwipeN’Swap. We have worked together over the last few weeks to develop an app we like to call S\\n\\nThrift stores, right now, it’s the busiest time of the year, realistically, might be hard for us to find \\n\\n\\n\\n\\n\\nQUESTIONS TO CONTINUE INTERVIEWING WITH\\n\\nShowing customer base wireframe of our app\\n\\nFuture directions→ usability rating\\n\\nGo to crossroads/goodwill\\n\\nMaybe an online survey? \\n\\nEmphasize how/why our team is qualified-- mention how deborah is on our team and is our advisor\\n\\n\\n\\n—> maybe we should ask a specific question: do you ever trade clothes with people; when was the last time you traded clothes? \\n\\nA couple of us can go to thrift / small antique stores downtown and elevator pitch our idea; see if they’d be down.\\n\\nRevenue Streams\\n\\n\\n\\nPremium memberships\\n\\nUnlimited right swipes\\n\\nCan see who swiped right on your clothes\\n\\nChat with anybody, regardless of if you matched with them (like + send a message) ‘super-like’\\n\\nYou can go back on something you accidentally swiped left on\\n\\n“Boost” algorithm to hype up your account\\n\\nMax doesn’t think stylists are viable; element of partnering with stylists is pretty antithetical to the simple mechanism of the app (niche, humble)--> ask for advice?\\n\\nAlso stylists will likely be premium members anyway\\n\\nThey’ll use it regardless, doesn’t have to be a whole separate element within the app\\n\\nPerhaps thrift stores and brands can have accounts—> people can sell or trade clothes? \\n\\nLocal businesses—> advertising us, could be like a trade (e.g., CookieCruz)\\n\\n\\n\\nAction Items:\\n\\n\\t-pursue 2-3 more interviews for the next week while incorporating the new \\n\\nquestions, maybe going to small stores/thrift stores\\n\\n\\t-Meet with Deborah eventually\\n\\n\\t-record in detail key insights \\n\\n\\t-maintain good communication with group\\n\\n\\t-potentially reach out to local business\\n\\n\\t-as far as returns go, we are 0 liability\\n\\nNext Steps: Meet tomorrow; discuss presentation', array([-0.01049833, -0.08939896, -0.03044511, ..., -0.03926747,\n",
            "       -0.03814263, -0.02892261], dtype=float32)], ['SNS4.docx', 'docx', 'Meeting Minutes\\n\\nFaceTime meeting 11PM 11/18\\n\\n\\n\\nAttendance:Dena, Katya, Max, Ashley, Max\\n\\n\\n\\nPLAN: Future directions, presentation for 11/19\\n\\n\\n\\nDiscussion\\n\\n\\n\\nDeborah\\n\\neffective pitch on slides\\n\\nSwipeN’Swap is a trading platform and it is a marketplace\\n\\nElevator pitch: Problem (HAVE ONE PROBLEM THOUGH! fashion is too expensive, college students=broke, we all have too much clothing-- fashion is a super dirty industry, get stats, clogging up landfills, college students don’t want to spend money), Value Proposition (the app is local, easy, convenient, user interface--> why are you better than the rest?), Business Solution\\n\\nHi, we are SwipeN’Swap. We have worked together over the last few weeks to develop a user-friendly app that allows users to easily and cheaply keep their wardrobe up to date through trading clothes, shoes, and accessories with people in their area. As college students collaborating with a sustainable fashion advisor, Deborah Lindsay, we have confidence in our ability to meet consumer needs when it comes to saving money while also staying fashionable. Find us on an app store near you soon!\\n\\nRemove thrift stores, right now, it’s the busiest time of the year, realistically, might be hard for us to find \\n\\n\\n\\nEmphasize how/why our team is qualified-- mention how deborah is on our team and is our advisor\\n\\n\\n\\nAction Items/Next Steps:\\n\\n\\t-Meet with Deborah 11/22 3:45PM\\n\\n\\t-record in detail key insights \\n\\n\\t-MAKE VIDEO on Wednesday!\\n\\n\\t-Make final 10 min presentation', array([-0.01049833, -0.08939896, -0.03044511, ..., -0.03926747,\n",
            "       -0.03814263, -0.02892261], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2i0aX-xbcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5byB46i4uU8",
        "colab_type": "code",
        "outputId": "da954cb1-68f2-48aa-9435-e9e3c84c4790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "initial = tarr[0][3]\n",
        "print(f'initial = {tarr[0][0]}')\n",
        "for t in tarr:\n",
        "  print(cosine(initial, t[3]),t[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial = Copy of AMS 131 Quiz 1 Responses.docx\n",
            "1.0000001 Copy of AMS 131 Quiz 1 Responses.docx\n",
            "0.7624198 Copy of Writing 2 #17 Blog - Rough Draft.docx\n",
            "0.9683681 Copy of AMS 131 Quiz 5 Responses.docx\n",
            "0.9873122 Copy of AMS 131 Quiz 2 Responses.docx\n",
            "0.9822319 Copy of AMS 131 Quiz 3 Responses.docx\n",
            "0.97915804 Copy of AMS 131 Quiz 4 Responses.docx\n",
            "0.9674914 Copy of AMS 131 Quiz 6 Responses.docx\n",
            "0.9623627 Copy of AMS 131 Quiz 7 Responses.docx\n",
            "0.96001863 Copy of AMS 131 Quiz 9 Responses.docx\n",
            "0.9470624 Copy of AMS 131 Quiz 10 Responses.docx\n",
            "0.96211 Copy of AMS 131 Quiz 8 Responses.docx\n",
            "0.7871333 Copy of Writing 2 #18 Blog - Final Draft.docx\n",
            "0.91894484 Copy of AMS 131 Test 1 Responses.docx\n",
            "0.8865065 Copy of AMS 131 Test 2 Responses(1).docx\n",
            "0.90783846 Copy of AMS 131 Test 3 Responses.docx\n",
            "0.7995126 Copy of Writing 2 #11 Topic Exploration.docx\n",
            "0.76722044 Copy of Writing 2 #15 Annotated Bibliography Responses.docx\n",
            "0.9107957 Copy of AMS 131 Test 2 Responses.docx\n",
            "0.7800787 Copy of Writing 2 #10 Rhetorical Analysis on Kasser(1).docx\n",
            "0.7831176 Copy of Writing 2 #15 Annotated Bibliography.docx\n",
            "0.90330964 Copy of Copy of AMS 131 Test 2 Responses.docx\n",
            "0.7751318 Copy of Writing 2 #10 Rhetorical Analysis on Kasser.docx\n",
            "0.7617378 Copy of Writing 2 #19 Metacognition Reflection on Group Presentation.docx\n",
            "0.77331185 Copy of Writing 2 #9 Rhetorical Analysis on Putnam(1).docx\n",
            "-0.26842695 SNS1.docx\n",
            "0.7603233 Copy of Writing 2 First Essay Second Draft.docx\n",
            "0.7775466 Copy of Writing 2 #9 Rhetorical Analysis on Putnam.docx\n",
            "0.7640124 Copy of Writing 2 #8B Metacognition Reflection on Essay #1.docx\n",
            "-0.26842695 SNS2.docx\n",
            "-0.26842695 SNS3.docx\n",
            "-0.26842695 SNS4.docx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj35tyMru_tB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "276ef484-2323-4281-bbed-3075b258ced0"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "\n",
        "def grab_file_names(path):\n",
        "\tfiles = []\n",
        "\tfor r, d, f in os.walk(path):\n",
        "\t\tfor file in f:\n",
        "\t\t\tfiles.append(os.path.join(r, file))\n",
        "\treturn files\n",
        "\n",
        "def K_means(n_clusters, big_dir):\n",
        "  files = grab_file_names(big_dir)\n",
        "  vecs = []\n",
        "  names = []\n",
        "  for f in files:\n",
        "    name = f.split(r'/'+big_dir+'/')[1]\n",
        "    print(name)\n",
        "    names.append(name)\n",
        "    vecs.append(get_doc2vec(name))\n",
        "  clf = KMeans(n_clusters=n_clusters, \n",
        "              max_iter=1000, \n",
        "              init='k-means++', \n",
        "              n_init=1)\n",
        "  labels = clf.fit_predict(vecs)\n",
        "  sets = [[] for _ in range(n_clusters)]\n",
        "  for i in range(len(names)):\n",
        "    sets[labels[i]].append(names[i])\n",
        "  return sets\n",
        "\n",
        "print(K_means(2, 'drive/My Drive/ApexioData'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copy of Writing 2 #17 Blog - Rough Draft.docx\n",
            "Copy of Writing 2 #18 Blog - Final Draft.docx\n",
            "Copy of Writing 2 #11 Topic Exploration.docx\n",
            "SNS1.docx\n",
            "SNS2.docx\n",
            "[['Copy of Writing 2 #17 Blog - Rough Draft.docx', 'Copy of Writing 2 #18 Blog - Final Draft.docx', 'Copy of Writing 2 #11 Topic Exploration.docx'], ['SNS1.docx', 'SNS2.docx']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k1UH9M4xS86",
        "colab_type": "code",
        "outputId": "a0159c7a-4073-4c74-fc65-454b9e8af1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "sets = [[] for _ in range(n_clusters)]\n",
        "for i in range(len(tarr)):\n",
        "  sets[labels[i]].append(tarr[i][0])\n",
        "for s in sets:\n",
        "  print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Copy of Writing 2 #17 Blog - Rough Draft.docx', 'Copy of Writing 2 #18 Blog - Final Draft.docx', 'Copy of Writing 2 #11 Topic Exploration.docx', 'Copy of Writing 2 #15 Annotated Bibliography Responses.docx', 'Copy of Writing 2 #10 Rhetorical Analysis on Kasser(1).docx', 'Copy of Writing 2 #15 Annotated Bibliography.docx', 'Copy of Writing 2 #10 Rhetorical Analysis on Kasser.docx', 'Copy of Writing 2 #19 Metacognition Reflection on Group Presentation.docx', 'Copy of Writing 2 #9 Rhetorical Analysis on Putnam(1).docx', 'Copy of Writing 2 First Essay Second Draft.docx', 'Copy of Writing 2 #9 Rhetorical Analysis on Putnam.docx', 'Copy of Writing 2 #8B Metacognition Reflection on Essay #1.docx']\n",
            "['SNS1.docx', 'SNS2.docx', 'SNS3.docx', 'SNS4.docx']\n",
            "['Copy of AMS 131 Quiz 1 Responses.docx', 'Copy of AMS 131 Quiz 5 Responses.docx', 'Copy of AMS 131 Quiz 2 Responses.docx', 'Copy of AMS 131 Quiz 3 Responses.docx', 'Copy of AMS 131 Quiz 4 Responses.docx', 'Copy of AMS 131 Quiz 6 Responses.docx', 'Copy of AMS 131 Quiz 7 Responses.docx', 'Copy of AMS 131 Quiz 9 Responses.docx', 'Copy of AMS 131 Quiz 10 Responses.docx', 'Copy of AMS 131 Quiz 8 Responses.docx', 'Copy of AMS 131 Test 1 Responses.docx', 'Copy of AMS 131 Test 2 Responses(1).docx', 'Copy of AMS 131 Test 3 Responses.docx', 'Copy of AMS 131 Test 2 Responses.docx', 'Copy of Copy of AMS 131 Test 2 Responses.docx']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zx_HzQ_xzun",
        "colab_type": "code",
        "outputId": "6816e58e-d3e7-4eaa-9250-d856cf16f72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stuff = glob.glob(root_path)\n",
        "stuff"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/ApexioData/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdWUGYMr-Hhv",
        "colab_type": "code",
        "outputId": "6812189d-5c0f-41cf-a6df-b72894ade711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "arr = os.listdir()\n",
        "arr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'encoder', 'drive', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhWtgLWRAiHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def grab_files(path):\n",
        "  files = []\n",
        "  for r, d, f in os.walk(path):\n",
        "      for file in f:\n",
        "          files.append(os.path.join(r, file))\n",
        "\n",
        "  return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HzX3U9h-cLs",
        "colab_type": "code",
        "outputId": "9123f10f-b59d-4b5d-db38-221d2c076dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "files = []\n",
        "dirs = []\n",
        "base_path = 'drive/My Drive/ApexioData/TestStuff/'\n",
        "for r, d, f in os.walk(base_path):\n",
        "  if len(d) != 0:\n",
        "    dirs += d\n",
        "\n",
        "print(dirs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['AMS', 'CS70', 'Science', 'History', 'Core1', 'Crown92', 'WesternCiv', 'English', 'CollegeApps', 'Writing 2', 'debatedata']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7pmtBZumlnWS",
        "colab": {}
      },
      "source": [
        "import re\n",
        "tarr = []\n",
        "vecs = []\n",
        "for i in range(len(dirs)):\n",
        "  dir = dirs[i]\n",
        "  print(dir)\n",
        "  files = grab_files(base_path+dir+'/')\n",
        "  for f in files:\n",
        "    r = f.split(r'/'+dir+'/')[1]\n",
        "    ext = r.split(r'.')[1]\n",
        "    extr = extract(f)\n",
        "    vecs.append(get_doc2vec(r))\n",
        "    tarr.append([r, ext, extr, vecs[-1],i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ2XC0HhA-Wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rand\n",
        "def generate_pairs(dat):\n",
        "  pairs = []\n",
        "  for i in range(len(dat)):\n",
        "    for j in range(i+1,len(dat)):\n",
        "      pairs.append((dat[i],dat[j]))\n",
        "  return pairs\n",
        "\n",
        "def generate_X_y_from_pairs(pairs):\n",
        "  X = [[],[]]\n",
        "  y = []\n",
        "  for pair in pairs:\n",
        "    X[0].append(pair[0][3])\n",
        "    X[1].append(pair[1][3])\n",
        "    y.append(1.0 if pair[0][4] == pair[1][4] else 0)\n",
        "  return [np.array(x) for x in X], np.array(y)\n",
        "\n",
        "def generate_X_y_from_dat(dat):\n",
        "  X = [[],[]]\n",
        "  y = []\n",
        "  for i in range(len(dat)):\n",
        "    for j in range(i+1,len(dat)):\n",
        "      X[0].append(dat[i][3])\n",
        "      X[1].append(dat[j][3])\n",
        "      y.append(1.0 if dat[i][4] == dat[j][4] else 0)\n",
        "  return [np.array(x) for x in X], np.array(y)\n",
        "\n",
        "pairs = generate_pairs(tarr)\n",
        "rand.shuffle(pairs)\n",
        "train_pairs = pairs[:int(len(pairs) * 0.8)]\n",
        "test_pairs = pairs[int(len(pairs) * 0.8):]\n",
        "X_train, y_train = generate_X_y_from_pairs(train_pairs)\n",
        "X_test, y_test = generate_X_y_from_pairs(test_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNj3vDThDU3B",
        "colab_type": "code",
        "outputId": "27cfa6a1-02f5-468c-9b08-ad0dab8c2ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJZt4jhJDXob",
        "colab_type": "code",
        "outputId": "0aa10d8d-2d36-4638-81a7-2fe4ee29b625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.layers import Input, Dense, Add, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GOjh8QVPYK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def get_dirs(path):\n",
        "  dirs = []\n",
        "  for r, d, f in os.walk(path):\n",
        "    if len(d) != 0:\n",
        "      dirs += d\n",
        "  return dirs\n",
        "\n",
        "def get_files(path, dirs):\n",
        "  tarr = []\n",
        "  vecs = []\n",
        "  for i in range(len(dirs)):\n",
        "    dir = dirs[i]\n",
        "    print(dir)\n",
        "    files = grab_files(path+dir+'/')\n",
        "    for f in files:\n",
        "      r = f.split(r'/'+dir+'/')[1]\n",
        "      ext = r.split(r'.')[1]\n",
        "      try:\n",
        "        extr = extract(f)\n",
        "        vecs.append(get_doc2vec(extr))\n",
        "        tarr.append([r, ext, extr, vecs[-1],dir])\n",
        "      except:\n",
        "        print(f'File {r} not readable')\n",
        "  return tarr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P20o5cMkPxYg",
        "colab_type": "code",
        "outputId": "b010b396-f47e-4067-8b99-b2a016d968e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(get_dirs(base_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['AMS', 'CS70', 'Science', 'History', 'Core1', 'Crown92', 'WesternCiv', 'English', 'CollegeApps', 'Writing 2', 'debatedata']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eGzlQX6P2f9",
        "colab_type": "code",
        "outputId": "14511dd3-880b-41a1-a9bc-78aa22814602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "dirs = get_dirs('drive/My Drive/ApexioData/Archive/')\n",
        "files_1 = get_files('drive/My Drive/ApexioData/Archive/',dirs)\n",
        "dirs = get_dirs('drive/My Drive/ApexioData/TestStuff/')\n",
        "files_2 = get_files('drive/My Drive/ApexioData/TestStuff/',dirs)\n",
        "dirs = get_dirs('drive/My Drive/ApexioData/DylanData/')\n",
        "files_3 = get_files('drive/My Drive/ApexioData/DylanData/',dirs)\n",
        "print('Making Pairs')\n",
        "pairs = generate_pairs(files_1 + files_2 + files_3)#pairs_1 + pairs_2\n",
        "rand.shuffle(pairs)\n",
        "train_pairs = pairs[:int(len(pairs) * 0.8)]\n",
        "test_pairs = pairs[int(len(pairs) * 0.8):]\n",
        "X_train, y_train = generate_X_y_from_pairs(train_pairs)\n",
        "X_test, y_test = generate_X_y_from_pairs(test_pairs)\n",
        "X_whole, X_whole = generate_X_y_from_pairs(pairs)\n",
        "whole_pairs = pairs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cornell\n",
            "APUSH\n",
            "Resume\n",
            "bioe100 final\n",
            "bio\n",
            "AMS\n",
            "CS70\n",
            "Science\n",
            "History\n",
            "Core1\n",
            "Crown92\n",
            "WesternCiv\n",
            "English\n",
            "File Archetypal Narrative Packet.doc not readable\n",
            "File ATSS Quick Write.doc not readable\n",
            "File Sashimi Cashmere.doc not readable\n",
            "File Root Practice.doc not readable\n",
            "File Identity Project Visual copy.jpg not readable\n",
            "File Root Practice Student.doc not readable\n",
            "CollegeApps\n",
            "Writing 2\n",
            "debatedata\n",
            "File 340824.pdf not readable\n",
            "File jlf71019.pdf not readable\n",
            "Phil 11\n",
            "Phil 8\n",
            "Stev Core 1\n",
            "Stev 26\n",
            "Making Pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IrFL7rSD-V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_1 = Input((4096,), dtype=tf.float32)\n",
        "input_2 = Input((4096,), dtype=tf.float32)\n",
        "reduce_dim_1 = Dense(1000, activation='relu')\n",
        "con = Concatenate()([reduce_dim_1(input_1),reduce_dim_1(input_2)])\n",
        "x_1 = Dense(1000, activation='relu')(con)\n",
        "x_2 = Dense(500, activation='relu')(x_1)\n",
        "x_3 = Dense(100, activation='relu')(x_2)\n",
        "x_4 = Dense(25, activation='relu')(x_3)\n",
        "out = Dense(1, activation='sigmoid')(x_4)\n",
        "dual_model = Model(inputs=[input_1, input_2], outputs=out) \n",
        "dual_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cwUA1kJGUWc",
        "colab_type": "code",
        "outputId": "66036bbe-31e9-4a87-9cce-cd3636c27bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "dual_model.fit(X_train, y_train, epochs=10)#,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36120 samples\n",
            "Epoch 1/10\n",
            "36120/36120 [==============================] - 6s 176us/sample - loss: 0.1865 - acc: 0.9318\n",
            "Epoch 2/10\n",
            "36120/36120 [==============================] - 6s 175us/sample - loss: 0.0886 - acc: 0.9643\n",
            "Epoch 3/10\n",
            "36120/36120 [==============================] - 6s 177us/sample - loss: 0.0510 - acc: 0.9804\n",
            "Epoch 4/10\n",
            "36120/36120 [==============================] - 6s 179us/sample - loss: 0.0366 - acc: 0.9872\n",
            "Epoch 5/10\n",
            "36120/36120 [==============================] - 7s 181us/sample - loss: 0.0275 - acc: 0.9906\n",
            "Epoch 6/10\n",
            "36120/36120 [==============================] - 7s 183us/sample - loss: 0.0225 - acc: 0.9927\n",
            "Epoch 7/10\n",
            "36120/36120 [==============================] - 6s 175us/sample - loss: 0.0191 - acc: 0.9935\n",
            "Epoch 8/10\n",
            "36120/36120 [==============================] - 6s 179us/sample - loss: 0.0247 - acc: 0.9913\n",
            "Epoch 9/10\n",
            "36120/36120 [==============================] - 6s 179us/sample - loss: 0.0252 - acc: 0.9913\n",
            "Epoch 10/10\n",
            "36120/36120 [==============================] - 6s 180us/sample - loss: 0.0170 - acc: 0.9947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f13ca6417b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqNYRg5vS9Wk",
        "colab_type": "code",
        "outputId": "9ff5fecc-9761-4f5d-8da2-6896a25ac0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dual_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9030/9030 [==============================] - 1s 94us/sample - loss: 0.0168 - acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01675312447191215, 0.9940199]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjIdvIRSGkga",
        "colab_type": "code",
        "outputId": "7b480319-a1bd-4803-8de6-dc2f113f23da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "correct = 0\n",
        "for pair in test_pairs:\n",
        "  X, y = generate_X_y_from_pairs([pair])\n",
        "  ev = round(np.sum(dual_model.predict(X)))\n",
        "  #print(pair[0][0] + ' -- ' + pair[1][0] + ' --', ev)\n",
        "  correct += 1 if ev == y[0] else 0\n",
        "print(correct / len(test_pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9880398671096345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEAJLbaITLQG",
        "colab_type": "code",
        "outputId": "a2a286e7-ccb8-4d44-950b-02170b864843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "dirs = get_dirs('drive/My Drive/ApexioData/TestStuff2/')\n",
        "files_1 = get_files('drive/My Drive/ApexioData/TestStuff2/',dirs)\n",
        "test_pairs_2 = generate_pairs(files_1)\n",
        "rand.shuffle(test_pairs_2)\n",
        "X_test_2, y_test_2 = generate_X_y_from_pairs(test_pairs_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Birthday\n",
            "xmas\n",
            "Drama\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD0rUtIDswjk",
        "colab_type": "code",
        "outputId": "6bb812e5-f8e7-4f75-df54-ae5e86b657d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dual_model.evaluate(X_test_2,y_test_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "171/171 [==============================] - 0s 129us/sample - loss: 1.6721 - acc: 0.6667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.672113857771221, 0.6666667]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 556
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRt8HEMGs7-N",
        "colab_type": "code",
        "outputId": "c4106098-cbe9-4293-edd0-7c5a66d2ca34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "correct = 0\n",
        "for pair in test_pairs_2:\n",
        "  X, y = generate_X_y_from_pairs([pair])\n",
        "  ev = round(np.sum(dual_model.predict(X)))\n",
        "  correct += 1 if ev == y[0] else 0\n",
        "  #if y[0] != ev:\n",
        "    #print(pair[0][0] + ' -- ' + pair[1][0] + ' --', ev)\n",
        "print(correct / len(test_pairs_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49707602339181284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPoEyoFZtLRX",
        "colab_type": "code",
        "outputId": "e7dd9b71-72f0-4cf9-d9cb-a88d579e1554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip3 install networkX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: networkX in /usr/local/lib/python3.6/dist-packages (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkX) (4.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu-uIgME6Jqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgTLIqZ_7LrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_graph(pairs):\n",
        "  g = nx.Graph()\n",
        "  X, y = generate_X_y_from_pairs(pairs)\n",
        "  out = dual_model.predict(X)\n",
        "  for i in range(len(pairs)):\n",
        "    if out[i][0] > 0.5:\n",
        "      g.add_edge(pairs[i][0][0], pairs[i][1][0])\n",
        "    else:\n",
        "      g.add_node(pairs[i][0][0])\n",
        "      g.add_node(pairs[i][1][0])\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwaMNJYJ7p0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = generate_graph(whole_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZIUcZUH7tWR",
        "colab_type": "code",
        "outputId": "724414e4-f52c-47db-c132-b621805ea442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "nx.draw(g)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsZPzLWI8df8",
        "colab_type": "code",
        "outputId": "cef5abb2-2fdc-4775-e852-a298eb9f5f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "for c in nx.connected_components(g):\n",
        "  print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ATSS Blog Post 2 V2.docx', 'Copy of APUSH DBQ1 Prep.docx', \"A Room of One's Own Chapters 3 and 6 Reading Notes.docx\", 'Lession 23.docx', 'Brooks BN Neg 4.docx', 'Genesis Reading Notes.docx', 'Diplomacy Definitions.docx', 'Body Paragraph Practice.docx', 'Absolute Monarch Interview.docx', 'Voltaire Research Paper Rough Draft.docx', 'Communist Manifesto Part 1 & 2 Reading Notes.docx', \"Olivia's Study Guide.docx\", 'Ch. 5 Discussion and Creative Writing.docx', 'Paradigm Shift Paragraph.docx', 'American Sniper Analysis.docx', 'Frq Re-wrte #2 APUSH.docx', 'Brooks BN Foundation Blocks.docx', 'Civil Disobedience Reading Notes.docx', 'Apush essay redo.docx', 'Kehler - Core Essay Topic I.pdf', 'Macbeth Act 2-3 Reading Guide.docx', 'PostWWIIdomesticpolicyquestions.doc.docx', 'Serial Essay Rough Draft.docx', 'Industrialization Essay.docx', 'Diplomacy England Research.docx', 'Second Essay Rough Draft.docx', 'First Essay Brain Storm.docx', '20032043.pdf', 'Brooks BN Nats Neg 2.docx', 'Proggressive Project.docx', 'ATSS Blog Post 2 comments.docx', 'Borderlands Reading Notes.docx', 'Great Gatsby Essay Thought Exploration 2.docx', 'Serial Essay Rough Draft v2.docx', 'Arguement Practice Paragraph.docx', 'Worksheet for 5th Period APUSH National Economy Group.docx', \"A Room of One's Own Chapters 1 and 2 Reading Notes.docx\", 'Identity Project Visual Explanationsv2.docx', 'Second Essay Final Draft.docx', 'Tao Te Ching Reading Notes.docx', 'ATSS Blog Post 4.docx', 'Emerging Nationailisim Lesson 5 Part B.docx', 'Conclusion Practice.docx', 'SBAC Outline.docx', 'Third Essay Final Draft.docx', 'Macbeth Act 4-5 Reading Guide.docx', 'The Answer Reading Notes.docx', \"Man's Search For Meaning Reading Notes.docx\", 'Final Essay Final Draft.docx', 'Voltaire Research Paper Final Draft.docx', 'Apology Reading Notes.docx', 'Argument Practice Paragraph ReSub v2.docx', 'Creating Theme Paragraph v3.docx', 'Serial Essay Rough Draft v3.docx', 'Existentialism Cafe Reading Notes.docx', 'Holocaust Paper Bibliography.docx', 'ColdWarTimelineAP.doc.docx', 'SOAPS APUSH.docx', 'Fable Outline.docx', 'Crito Reading Notes.docx', 'ATSS Blog Post 1.docx', 'Creating Theme Paragraph v2.docx', 'Japanese Internment Paragraph.docx', 'w7616.pdf', 'ATSS Blog Post 4 Final.docx', 'Lesson 21 B.docx', 'ATSS Blog Post 2 Final.docx', 'PresidentialForeignPolicyQuestions.doc.docx', 'ATSS Blog Post 4 with comments.docx', '3 Word Story.docx', 'Identity Project Visual Citations.docx', 'Great Gatsby Bullshit Essay.docx', 'New Right Essay Revision.docx', 'White Privilege Reading Notes.docx', 'Civilization and its Discontents Reading Notes.docx', 'American Sniper Analysisv2.docx', 'ATSS Blog Post 3 Final.docx', 'Journel Assignment #2.docx', 'APUSH Question Lesson 11.docx', 'APUSH Summer Large Essays_.docx', 'New Right Rough Draft.docx', \"The World's Religions Reading Notes.docx\", 'PostWWIIDomesticPolicybyPresident.doc.docx', 'Brooks BN Neg New.docx', 'Holocaust Paper Rough Draft.docx', 'Second Essay Arguments.docx', 'Fable.docx', \"Voltaire Research Paper Bibliography (Christine Alfano's conflicted copy 2017-02-04).docx\", 'The Wealth of Nations Reading Notes.docx', 'Voltaire Research Paper Bibliography 2.0.docx', 'New Right Essay Revision 2.docx', 'j.0967-0750.2004.00170.x.pdf', 'AnteBellum- Reflection Questions.docx', 'Southern Manifesto Summary.docx', 'Hosseini Writing.docx', 'ATSS Blog Post 3.docx', 'MacBeth Act 1 Reading Guide.docx', '0895330053147949.pdf', 'Brooks BN Nats Neg STOCK.docx', 'Identity Project Visual Explanations.docx', 'Semester 1 Study Guide.docx', 'Harris Research.docx', 'Emerging Nationalisim and its Significance.docx', 'ColdWarForeignPolicybyPresident.doc.docx', 'The Answer Textual Analysis.docx', 'Journel Assignment #1.docx', 'Top 3 causes of the Civil War.docx', 'Creating Theme Paragraph.docx', 'Max Alfano-Smith Industrialization Essay.docx', 'Kehler Core Syllabus Fall 2018.pdf', 'APUSH Summer Short Essays.docx', 'Great Gatsby Essay Thought Exploration.docx', \"Man's Search For Meaning Reflection.docx\", 'The 13th Reflection.docx', 'Holocaust Paper.docx', 'Essay 1 Quick Writes.docx', 'Great Gatsby Paragraph Outline.docx', 'Nietzsche First Essay Reading Notes.docx', 'Existentialism Reading Notes.docx', 'Chapter 1 APUSH.docx', 'Brooks BN Nats Aff 2.docx', 'Sister Citizen Reading Notes.docx', \"Voltaire Research Paper Rough Draft (Max Alfano-Smith's conflicted copy 2015-11-22).docx\", '41408737.pdf', 'Alfano-Smith, Max Scored Essay.docx', 'FRQ 2 Graphic Organizers.docx', 'ATSS Blog Post 2.docx', 'Voltaire Research Paper Bibliography.docx', 'Progressive Project Written.docx', 'Arguement Practice Paragraph v2.docx', 'Mathew Reading Notes.docx', 'ATSS Blog Post 3 V2.docx', 'wp867 .pdf', 'Lesson 18 B.docx', 'Macbeth Essay.docx', 'Voltaire Article.docx', 'Roosevelt Paragraphs.docx', 'ATSS Blog Post 1v2.docx', 'Brooks BN Nats Aff STOCK.docx', 'Diplomacy Questions.docx', 'Holocaust Paper Rough Draft with comments.docx', 'Second Essay Brain Storm.docx', 'Tao Te Ching 81 Summary.docx', 'First Essay Rough Draft 6.docx', 'Arguement Practice Paragraph ReSub v2.docx', 'Industrial Revolution In-Class Essay.docx', 'sdarticle.pdf', 'ATSS Blog Post 4 V2.docx', 'Exodus and Mathew Reading Notes.docx', 'APUSH Orgin and Devolopment of Slavery paragraph 1.docx', 'Theocracy Presentation Pro Arguements.docx', 'Brooks BN Aff 3.docx', 'Arguement Practice Paragraph ReSub.docx', 'ATSS Blog Post 3 with comments.docx', 'Rebellion Comparision Paragraph.docx'}\n",
            "{'Vikranth_s_Resume_Oct_8_3.pdf', 'Vikranth_s Resume(3).docx', 'Vikranth Resume.docx', 'Main Vikranth Resume.docx', 'Resume V2 shorter.docx', 'VikranthsMainResume.docx', 'Pre - Resume.docx', 'Resume(3).docx', 'Resume(2).docx', \"Vikranth's Resume.docx\", 'Vikranth-Resume.docx', 'VikranthsResume.pdf', 'Main Vikranth Resume 3.docx', 'Vikranth_s Resume.docx', 'Vikranth_s Resume(1).docx', 'Resume.pdf', 'Resume(1).docx', 'VikranthsMainResume(1).docx', 'Resume(4).docx', 'Main Vikranth Resume 2.docx', 'Vikranth_s Resume(2).docx', 'Vikranth_s_Resume.pdf', 'VikranthsMainResume(2).docx', \"Vikranth's Resume (1).docx\", 'Resume-Diversatech.docx', 'Resume.docx'}\n",
            "{'Stev 1 Freud Reading Response.docx', 'Stev 1 Final Paper.docx', 'Stev 1 Reading Response.docx'}\n",
            "{'Phil 11 Lecture Notes 1.docx', 'Phil 11 Reading Response Symposium.docx', 'Phil 11 Reading Response Journal_ Purcell’s “Life on the Slippery Earth”.docx', 'Pre-Lecture Reflection for _Zen Buddhism (continued)_.docx', 'Phil 11 “Plato’s Academy” Lecture Notes.docx', 'PHIL 11 Pre Lecture 2.docx', 'PHIL 11 Lecture Notes 2 _Sophists _ The Figure of Socrates_.docx', 'Phil 11 _Hellenistic Schools_ Epicureanism _ Stoicism_ Lecture Notes.docx', 'Phill 11 Reading Response Value of Philosophy.docx', 'Reading Response Journal_ Seneca_s Moral Letters to Lucilius.docx', 'Aristotle_s Nicomachean Ethics Reading Response.docx', 'Phill 11 Pre-Lecture Reflection for _Hellenistic Schools_ Introduction, Cynicism _ Skepticism_.docx', 'Phil 11 Reading Response Journal_ Nietzsche_s The Gay Science.docx', 'Phil 11 Reading Response Journal_ _Discourse on the Noble Quest_.docx', 'Phil 11 Pre-Lecture Reflection for _Introduction to Zen Buddhism_.docx', 'Phil 11 Pre Lecture Platos Academy.docx', 'Phil 11 Reading Response Journal_ Tao Te Ching (second half).docx', 'Phil 11 Pre Lecture 1.docx', 'Phil 11 Pre-Lecture Reflection for _Hellenistic Schools_ Epicureanism _ Stoicism_.docx', 'Pre-Lecture Reflection for _Early Buddhism_.docx', 'Reading Response Journal_ _Discourse to the Kalamas_ _ The Greater Discourse on Cause_.docx', 'Pre-Lecture Reflection for _Spiritual Exercises _ Concentration on the Self_.docx', 'Phil 11 Reading Response Journal_ Tao Te Ching (first half).docx'}\n",
            "{'n6.pdf', ' Part 3 - Final Reflection.docx', 'Copy of AMS 131 Test 2 Responses.docx', 'n0.pdf', 'n12.pdf', 'n7.pdf', 'n3.pdf', 'Copy of AMS 131 Quiz 2 Responses.docx', 'Copy of AMS 131 Quiz 8 Responses.docx', 'Copy of AMS 131 Quiz 4 Responses.docx', 'Copy of AMS 131 Quiz 1 Responses.docx', 'n2.pdf', 'Copy of Copy of AMS 131 Test 2 Responses.docx', 'Copy of AMS 131 Quiz 10 Responses.docx', 'n17.pdf', 'Copy of AMS 131 Test 1 Responses.docx', 'Copy of AMS 131 Quiz 6 Responses.docx', 'n9.pdf', 'Case Study Bioe 100 Part 3.docx', 'Copy of AMS 131 Test 3 Responses.docx', 'Copy of AMS 131 Quiz 3 Responses.docx', 'n1.pdf', 'n11.pdf', 'Copy of AMS 131 Test 2 Responses(1).docx', 'Copy of AMS 131 Quiz 5 Responses.docx', 'n15.pdf', 'n16.pdf', 'n8.pdf', 'n18.pdf', 'n5.pdf', 'n19.pdf', ' Part 4 - Final Self-Assessment.docx', 'n4.pdf', 'Copy of AMS 131 Quiz 7 Responses.docx', 'n14.pdf', 'Copy of AMS 131 Quiz 9 Responses.docx', 'n10.pdf', 'Code of Ethics.docx', 'n21.pdf', 'n13.pdf'}\n",
            "{'Dartmouth App Questions 2.docx', 'UC Application Questions Revision 4.docx', \"UC Boulder Application Version That Isn't Fucking Cancer v2.docx\", 'Claremont Mckenna App Questions.docx', 'Pomona App Questions 3.docx', 'USC App Questions 2.docx', 'Common App Personal Response Question Revision 1.docx', 'Stanford App Questions2.docx'}\n",
            "{'Phil 8 week 2 forum.docx', 'Phil 8 Discussion 1.docx', 'Phil 8 Midterm Study Guide.docx', 'Phil 8 Week 4 Forum.docx', 'Phil 8 Forum Post 5.docx'}\n",
            "{'Lab #20 Report.docx', 'Autobiography.docx', 'Atomic Timeline Essay.docx', 'Sewer Science Lab Report.docx', 'Gas Mystery Paragraph.docx', 'Potato Lab Report.docx', 'Catalytic Activity DQs.docx', 'Singing bar stuff.docx', 'A mole of moles reflection.docx', 'Lab 46 Response.docx', 'Lab 46-47 Report.docx', 'Element BS.docx', 'Final Real Compared To What.docx'}\n",
            "{'SNS3.docx', 'SNS2.docx', 'SNS1.docx', 'SNS4.docx'}\n",
            "{'Copy of Writing 2 #10 Rhetorical Analysis on Kasser.docx', 'Copy of Writing 2 #9 Rhetorical Analysis on Putnam.docx', 'Copy of Writing 2 #15 Annotated Bibliography Responses.docx', 'Copy of Writing 2 First Essay Second Draft.docx', 'Copy of Writing 2 #11 Topic Exploration.docx', 'Copy of Writing 2 #17 Blog - Rough Draft.docx', 'Copy of Writing 2 #10 Rhetorical Analysis on Kasser(1).docx', 'Copy of Writing 2 #8B Metacognition Reflection on Essay #1.docx', 'Copy of Writing 2 #15 Annotated Bibliography.docx', 'Copy of Writing 2 #19 Metacognition Reflection on Group Presentation.docx', 'Copy of Writing 2 #9 Rhetorical Analysis on Putnam(1).docx', 'Copy of Writing 2 #18 Blog - Final Draft.docx'}\n",
            "{'El projecto de Science_.docx', 'Copy of Bio final.docx', 'Copy of Philip_s 2 Biology Final Study Guide.docx', \"Philip's 2 Biology Final Study Guide.docx\"}\n",
            "{' Cornell short Responses December 16th.docx', 'Cornell short Responses December 22th Version 2.docx', 'Cornell short Responses.docx', 'Cornell short Responses December 22th.docx', 'Cornell short Responses December 20th.docx'}\n",
            "{'Stev 26 Diversity.docx'}\n",
            "{'Stevenson Core Summer Assignment.docx'}\n",
            "{'Explanation between the relationship of orginisom to abotic factors.docx'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDKPRusS-yQd",
        "colab_type": "code",
        "outputId": "c42790ab-7389-427c-cc1a-0816e167a5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "g.order()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "301"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuv4--4eCPRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dual_model.deploy_model(description='Keras Dual Input Data Organization Model',\n",
        "            author=\"Maximillian Alfano-Smith and Roanak Baviskar\", organisation='Apexio',\n",
        "            file_name='dual_model.sav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSbjdrG6mjKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dual_model.save('Model.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN4Enxwqn9Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}