# -*- coding: utf-8 -*-
"""Copy of Apexio.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1gHfA3vScO_ZdhSZErH3AXmHq011DM8Pr

extensions to edit: .pdf, .txt, .docx
"""

# Commented out IPython magic to ensure Python compatibility.
# import stuff
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

import numpy as np
import random as rand
import os, re, json, nltk, argparse, random, torch, pickle
import tensorflow as tf
import tensorflow_hub as hub
import json
from random import randint
from tensorflow.keras.layers import Input, Dense, Add, Dropout, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from models import InferSent
from sklearn.cluster import KMeans
import textract

model = None

def get_doc2vec(text, model):
	sents = nltk.sent_tokenize(text)
	emb = model.encode(sents)
	sum = None
	for e in emb:
		if sum is None:
			sum = e
		else:
			sum += e
	return sum / len(sents)

def generate_pairs(dat):
	pairs = []
	for i in range(len(dat)):
		for j in range(i+1,len(dat)):
			pairs.append((dat[i],dat[j]))
	return pairs

def generate_X_y_from_pairs(pairs):
	X = [[],[]]
	y = []
	for pair in pairs:
		X[0].append(pair[0][3])
		X[1].append(pair[1][3])
		y.append(1.0 if pair[0][4] == pair[1][4] else 0)
	return [np.array(x) for x in X], np.array(y)

def generate_X_y_from_dat(dat):
	X = [[],[]]
	y = []
	for i in range(len(dat)):
		for j in range(i+1,len(dat)):
			X[0].append(dat[i][3])
			X[1].append(dat[j][3])
			y.append(1.0 if dat[i][4] == dat[j][4] else 0)
	return [np.array(x) for x in X], np.array(y)

def generate_X_y_for_dir(dat, dir_name):
	X = []
	y = []
	for i in range(len(dat)):
			X[0].append(dat[i][3])
			y.append(1.0 if dat[i][4] == dir_name else 0)
	return np.array(X), np.array(y)

def get_dirs(path):
	dirs = []
	for r, d, f in os.walk(path):
		if len(d) != 0:
			dirs += d
	return dirs

def generate_quads(names, contents, dirs):
	tarr = []
	for i in range(len(names)):
		ext = names[i].split(r'.')[1]
		try:
			vecs.append(get_doc2vec(contents[i]))
			tarr.append([names[i], ext, contents[i], vecs[-1],dirs[i]])
		except:
			print(f'File {r} not readable')
	return tarr

def generate_model():
	input_1 = Input((4096,), dtype=tf.float32)
	reduce_dim_1 = Dense(2000, activation='relu')(input_1)
	x_1 = Dense(1000, activation='relu')(reduce_dim_1)
	x_2 = Dense(500, activation='relu')(x_1)
	x_3 = Dense(100, activation='relu')(x_2)
	x_4 = Dense(25, activation='relu')(x_3)
	out = Dense(1, activation='sigmoid')(x_4)
	dual_model = Model(inputs=input_1, outputs=out)
	dual_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
	return dual_model

def grab_files(path):
	files = []
	for r, d, f in os.walk(path):
		for file in f:
			files.append(os.path.join(r, file))
	return files

def extract(file_path):
    text = textract.process(file_path)
    return text.decode('utf-8')

def get_file(path, model):
	try:
		extr = extract(path)
		return [path, 'NAN', 'NAN', get_doc2vec(extr, model)]
	except:
		return [path, 'NAN', 'NAN', None]
	return None

def generate_model():
	input_1 = Input((4096,), dtype=tf.float32)
	reduce_dim_1 = Dense(2000, activation='relu')(input_1)
	x_1 = Dense(1000, activation='relu')(reduce_dim_1)
	x_2 = Dense(500, activation='relu')(x_1)
	x_3 = Dense(100, activation='relu')(x_2)
	x_4 = Dense(25, activation='relu')(x_3)
	out = Dense(1, activation='sigmoid')(x_4)
	dual_model = Model(inputs=input_1, outputs=out)
	dual_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
	return dual_model

# ('Path','id','Model_Location')
def make_predictions(dir_triples, file_paths, model):

	nltk.download('punkt')

	# This is the big file so run this once to download to your drive
	#!mkdir 'drive/My Drive/ApexioData/GloVe'
	#!curl -Lo 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.zip' http://nlp.stanford.edu/data/glove.840B.300d.zip
	#!unzip 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.zip' -d 'drive/My Drive/ApexioData/GloVe/'

	# Run every time to download on the colab runtime
	#!mkdir encoder
	#!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl

	# Load model

	#model_version = 1
	#MODEL_PATH = "encoder/infersent%s.pkl" % model_version
	#params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,
					#'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}
	#model = InferSent(params_model)
	#model.load_state_dict(torch.load(MODEL_PATH))

	#model = model.cuda()

	#W2V_PATH = 'drive/My Drive/ApexioData/GloVe/glove.840B.300d.txt'
	#model.set_w2v_path(W2V_PATH)

	#model.build_vocab_k_words(K=100000)

	file_contents_non_null = []
	file_contents_null = []
	vecs = []
	for path in file_paths:
		cont = get_file(path, model)
		if cont[3] is None:
			file_contents_null.append(cont)
		else:
			file_contents_non_null.append(cont)
			vecs.append(cont[3])

	predictions = [[] for _ in range(len(file_contents_non_null))]
	for triple in dir_triples:
		try:
			mode = generate_model()
			mode.load_weights(triple[2])
			preds = mode.predict(np.array(vecs))
			for i in range(len(preds)):
				predictions[i].append(preds[i][0])
		except Exception as error:
			for i in range(len(vecs)):
				predictions[i].append(-1)

	recommendations = []
	#print('Recommendations')
	for file in file_contents_null:
		#print(file[0], ': None')
		recommendations.append((file[0], None))
	for i in range(len(file_contents_non_null)):
		#print(file_contents_non_null[i][0],':',dir_triples[argmax(predictions[i])][1])
		recommendations.append((file_contents_non_null[i][0],dir_triples[np.argmax(predictions[i])][1]))
	return recommendations

def grab_file_names(path):
	files = []
	for r, d, f in os.walk(path):
		for file in f:
			files.append(os.path.join(r, file))
	return files

def K_means(n_clusters, big_dir):
	files = grab_file_names(big_dir)
	vecs = []
	names = []
	for f in files:
		name = f.split(r'/'+big_dir+'/')[1]
		print(name)
		names.append(name)
		vecs.append(get_doc2vec(name))
	clf = KMeans(n_clusters=n_clusters,
	          max_iter=1000,
	          init='k-means++',
	          n_init=1)
	labels = clf.fit_predict(vecs)
	sets = [[] for _ in range(n_clusters)]
	for i in range(len(names)):
		sets[labels[i]].append(names[i])
	return sets
